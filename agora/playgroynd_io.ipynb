{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/HJP/KUAICVLab/Hand/AbsoluteTrack\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/HJP/KUAICVLab/Hand/AbsoluteTrack'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "img = np.zeros((100, 100), dtype=np.uint8)\n",
    "plt.imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import os\n",
    "\n",
    "import av\n",
    "import fnmatch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import lib.data_utils.fs as fs\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# from lib.tracker.perspective_crop import landmarks_from_hand_pose\n",
    "\n",
    "\n",
    "#from multiprocessing import Pool\n",
    "#from typing import Optional, Tuple\n",
    "\n",
    "from lib.models.model_loader import load_pretrained_model\n",
    "\n",
    "#from lib.tracker.tracker import HandTracker, HandTrackerOpts, InputFrame, ViewData\n",
    "from lib.tracker.video_pose_data import SyncedImagePoseStream\n",
    "\n",
    "from typing import Dict, NamedTuple\n",
    "\n",
    "#from lib.common.hand import NUM_HANDS, NUM_LANDMARKS_PER_HAND\n",
    "NUM_HANDS = 2\n",
    "NUM_LANDMARKS_PER_HAND = 21\n",
    "NUM_FINGERTIPS_PER_HAND = 5\n",
    "NUM_JOINTS_PER_HAND = 22\n",
    "LEFT_HAND_INDEX = 0\n",
    "RIGHT_HAND_INDEX = 1\n",
    "\n",
    "NUM_DIGITS: int = 5\n",
    "NUM_JOINT_FRAMES: int = 1 + 1 + 3 * 5  # root + wrist + finger frames * 5\n",
    "DOF_PER_FINGER: int = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import from lib.common.crop.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Tuple\n",
    "\n",
    "import lib.common.camera as camera\n",
    "import numpy as np\n",
    "\n",
    "#from . import affine\n",
    "import lib.common.affine as affine\n",
    "\n",
    "\n",
    "def gen_intrinsics_from_bounding_pts(\n",
    "    pts_eye: np.ndarray, image_w: int, image_h: int, min_focal: float = 5\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    pts_ndc = pts_eye[..., 0:2] / pts_eye[..., 2:]\n",
    "    img_size = np.array([image_w, image_h], dtype=pts_eye.dtype)\n",
    "    # Given our convention, we need to shift one pixel before dividing by 2.\n",
    "    cx_cy = (img_size - 1) / 2\n",
    "    fx_fy = cx_cy / np.absolute(pts_ndc).max()\n",
    "\n",
    "    # Some sanity checks\n",
    "    if np.any(pts_eye[..., 2:] < 0.0001) or np.any(fx_fy < min_focal):\n",
    "        raise ValueError(\"Unable to create crop camera\", fx_fy)\n",
    "\n",
    "    return fx_fy, cx_cy\n",
    "\n",
    "\n",
    "def gen_crop_parameters_from_points(\n",
    "    camera_orig: camera.CameraModel,\n",
    "    pts_world,\n",
    "    new_image_size: Tuple[int, int],\n",
    "    mirror_img_x: bool,\n",
    "    camera_angle: float = 0,\n",
    "    focal_multiplier: float = 0.95,\n",
    ") -> camera.PinholePlaneCameraModel:\n",
    "    \"\"\"\n",
    "    Given the original camera transform and a list of 3D points in the world space,\n",
    "    compute the new perspective camera that makes sure after projection all the points\n",
    "    can be projected inside the image.\n",
    "\n",
    "    Auguments:\n",
    "    * camera_orig: the original camera used for generating an image. The returned camera\n",
    "        will have the same position but different rotation and intrinsics parameters.\n",
    "    * pts_world: points in the world space that must be projected inside the image by\n",
    "        the generated world to eye transform and intrinsics.\n",
    "    * new_image_size: target image size\n",
    "    * mirror_img_x: whether to flip the image. A typical use case is we usually mirror the\n",
    "        right hand images so that a model need to handle left hand data only\n",
    "    * camera_angle: how the camera is oriented physically so that we can rotate the object of\n",
    "        interest to the 'upright' direction\n",
    "    * focal_multiplier: when less than 1, we are zooming out a little. The effect on the image\n",
    "        is some margin will be left at the boundary.\n",
    "    \"\"\"\n",
    "    orig_world_to_eye_xf = np.linalg.inv(camera_orig.camera_to_world_xf)\n",
    "\n",
    "    crop_center = (pts_world.min(axis=0) + pts_world.max(axis=0)) / 2.0\n",
    "    new_world_to_eye = affine.make_look_at_matrix(\n",
    "        orig_world_to_eye_xf, crop_center, camera_angle\n",
    "    )\n",
    "    if mirror_img_x:\n",
    "        mirrorx = np.eye(4, dtype=np.float32)\n",
    "        mirrorx[0, 0] = -1\n",
    "        new_world_to_eye = mirrorx @ new_world_to_eye\n",
    "\n",
    "    fx_fy, cx_cy = gen_intrinsics_from_bounding_pts(\n",
    "        affine.transform3(new_world_to_eye, pts_world),\n",
    "        new_image_size[0],\n",
    "        new_image_size[1],\n",
    "    )\n",
    "    fx_fy = focal_multiplier * fx_fy\n",
    "\n",
    "    return camera.PinholePlaneCameraModel(\n",
    "        width=new_image_size[0],\n",
    "        height=new_image_size[1],\n",
    "        f=fx_fy,\n",
    "        c=cx_cy,\n",
    "        distort_coeffs=[],\n",
    "        camera_to_world_xf=np.linalg.inv(new_world_to_eye),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import from lib.tracker.tracking_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, NamedTuple\n",
    "\n",
    "class SingleHandPose(NamedTuple):\n",
    "    \"\"\"\n",
    "    A hand pose is composed of two fields:\n",
    "    1) joint angles where # joints == # DoFs\n",
    "    2) root-to-world rigid wrist transformation\n",
    "    \"\"\"\n",
    "\n",
    "    joint_angles: np.ndarray = np.zeros(NUM_JOINTS_PER_HAND, dtype=np.float32)\n",
    "    wrist_xform: np.ndarray = np.eye(4, dtype=np.float32)\n",
    "    hand_confidence: float = 1.0\n",
    "\n",
    "\n",
    "# Tracking result maps from hand_index to hand_pose\n",
    "class TrackingResult(NamedTuple):\n",
    "    hand_poses: Dict[int, SingleHandPose] = {}\n",
    "    num_views: Dict[int, int] = {}\n",
    "    predicted_scales: Dict[int, float] = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import from lib.common.crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Tuple\n",
    "\n",
    "import lib.common.camera as camera\n",
    "import lib.common.affine as affine\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# from . import affine\n",
    "\n",
    "\n",
    "def gen_intrinsics_from_bounding_pts(\n",
    "    pts_eye: np.ndarray, image_w: int, image_h: int, min_focal: float = 5\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    pts_ndc = pts_eye[..., 0:2] / pts_eye[..., 2:]\n",
    "    img_size = np.array([image_w, image_h], dtype=pts_eye.dtype)\n",
    "    # Given our convention, we need to shift one pixel before dividing by 2.\n",
    "    cx_cy = (img_size - 1) / 2\n",
    "    fx_fy = cx_cy / np.absolute(pts_ndc).max()\n",
    "\n",
    "    # Some sanity checks\n",
    "    if np.any(pts_eye[..., 2:] < 0.0001) or np.any(fx_fy < min_focal):\n",
    "        raise ValueError(\"Unable to create crop camera\", fx_fy)\n",
    "\n",
    "    return fx_fy, cx_cy\n",
    "\n",
    "\n",
    "def gen_crop_parameters_from_points(\n",
    "    camera_orig: camera.CameraModel,\n",
    "    pts_world,\n",
    "    new_image_size: Tuple[int, int],\n",
    "    mirror_img_x: bool,\n",
    "    camera_angle: float = 0,\n",
    "    focal_multiplier: float = 0.95,\n",
    ") -> camera.PinholePlaneCameraModel:\n",
    "    \"\"\"\n",
    "    Given the original camera transform and a list of 3D points in the world space,\n",
    "    compute the new perspective camera that makes sure after projection all the points\n",
    "    can be projected inside the image.\n",
    "\n",
    "    Auguments:\n",
    "    * camera_orig: the original camera used for generating an image. The returned camera\n",
    "        will have the same position but different rotation and intrinsics parameters.\n",
    "    * pts_world: points in the world space that must be projected inside the image by\n",
    "        the generated world to eye transform and intrinsics.\n",
    "    * new_image_size: target image size\n",
    "    * mirror_img_x: whether to flip the image. A typical use case is we usually mirror the\n",
    "        right hand images so that a model need to handle left hand data only\n",
    "    * camera_angle: how the camera is oriented physically so that we can rotate the object of\n",
    "        interest to the 'upright' direction\n",
    "    * focal_multiplier: when less than 1, we are zooming out a little. The effect on the image\n",
    "        is some margin will be left at the boundary.\n",
    "    \"\"\"\n",
    "    orig_world_to_eye_xf = np.linalg.inv(camera_orig.camera_to_world_xf)\n",
    "\n",
    "    crop_center = (pts_world.min(axis=0) + pts_world.max(axis=0)) / 2.0\n",
    "    new_world_to_eye = affine.make_look_at_matrix(\n",
    "        orig_world_to_eye_xf, crop_center, camera_angle\n",
    "    )\n",
    "    if mirror_img_x:\n",
    "        mirrorx = np.eye(4, dtype=np.float32)\n",
    "        mirrorx[0, 0] = -1\n",
    "        new_world_to_eye = mirrorx @ new_world_to_eye\n",
    "\n",
    "    fx_fy, cx_cy = gen_intrinsics_from_bounding_pts(\n",
    "        affine.transform3(new_world_to_eye, pts_world),\n",
    "        new_image_size[0],\n",
    "        new_image_size[1],\n",
    "    )\n",
    "    fx_fy = focal_multiplier * fx_fy\n",
    "\n",
    "    return camera.PinholePlaneCameraModel(\n",
    "        width=new_image_size[0],\n",
    "        height=new_image_size[1],\n",
    "        f=fx_fy,\n",
    "        c=cx_cy,\n",
    "        distort_coeffs=[],\n",
    "        camera_to_world_xf=np.linalg.inv(new_world_to_eye),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import from lib.tracker.perspective_crop.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import lib.common.camera as camera\n",
    "import numpy as np\n",
    "import torch\n",
    "#from lib.common.crop import gen_crop_parameters_from_points\n",
    "from lib.common.hand import HandModel, NUM_JOINTS_PER_HAND, RIGHT_HAND_INDEX\n",
    "from lib.common.hand_skinning import skin_landmarks\n",
    "\n",
    "#from .tracking_result import SingleHandPose\n",
    "\n",
    "\n",
    "def neutral_joint_angles(up: HandModel, lower_factor: float = 0.5) -> torch.Tensor:\n",
    "    joint_limits = up.joint_limits\n",
    "    assert joint_limits is not None\n",
    "    return joint_limits[..., 0] * lower_factor + joint_limits[..., 1] * (\n",
    "        1 - lower_factor\n",
    "    )\n",
    "\n",
    "\n",
    "def skin_landmarks_np(\n",
    "    hand_model: HandModel,\n",
    "    joint_angles: np.ndarray,\n",
    "    wrist_transforms: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    landmarks = skin_landmarks(\n",
    "        hand_model,\n",
    "        torch.from_numpy(joint_angles).float(),\n",
    "        torch.from_numpy(wrist_transforms).float(),\n",
    "    )\n",
    "    return landmarks.numpy()\n",
    "\n",
    "\n",
    "def landmarks_from_hand_pose(\n",
    "    hand_model: HandModel, hand_pose: SingleHandPose, hand_idx: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute 3D landmarks in the world space given the hand model and hand pose.\n",
    "    \"\"\"\n",
    "    xf = hand_pose.wrist_xform.copy()\n",
    "    # This function expects the user hand model to be a left hand.\n",
    "    if hand_idx == RIGHT_HAND_INDEX:\n",
    "        xf[:, 0] *= -1\n",
    "    landmarks = skin_landmarks_np(hand_model, hand_pose.joint_angles, xf)\n",
    "    return landmarks\n",
    "\n",
    "\n",
    "def rank_hand_visibility_in_cameras(\n",
    "    cameras: List[camera.CameraModel],\n",
    "    hand_model: HandModel,\n",
    "    hand_pose: SingleHandPose,\n",
    "    hand_idx: int,\n",
    "    min_required_vis_landmarks: int,\n",
    ") -> List[int]:\n",
    "    landmarks_world = landmarks_from_hand_pose(hand_model, hand_pose, hand_idx)\n",
    "    n_landmarks_in_view = []\n",
    "    ranked_cam_indices = []\n",
    "    for cam_idx, camera in enumerate(cameras):\n",
    "        landmarks_eye = camera.world_to_eye(landmarks_world)\n",
    "        landmarks_win2 = camera.eye_to_window(landmarks_eye)\n",
    "\n",
    "        n_visible = (\n",
    "            (landmarks_win2[..., 0] >= 0)\n",
    "            & (landmarks_win2[..., 0] <= camera.width - 1)\n",
    "            & (landmarks_win2[..., 1] >= 0)\n",
    "            & (landmarks_win2[..., 1] <= camera.height - 1)\n",
    "            & (landmarks_eye[..., 2] > 0)\n",
    "        ).sum()\n",
    "\n",
    "        n_landmarks_in_view.append(n_visible)\n",
    "        # Only push the cameras that can see enough hand points\n",
    "        if n_visible >= min_required_vis_landmarks:\n",
    "            ranked_cam_indices.append(cam_idx)\n",
    "\n",
    "    #  Favor the view that sees more landmarks\n",
    "    ranked_cam_indices.sort(\n",
    "        reverse=True,\n",
    "        key=lambda x: n_landmarks_in_view[x],\n",
    "    )\n",
    "    return ranked_cam_indices\n",
    "\n",
    "\n",
    "def _get_crop_points_from_hand_pose(\n",
    "    hand_model: HandModel,\n",
    "    gt_hand_pose: SingleHandPose,\n",
    "    hand_idx: int,\n",
    "    num_crop_points: int,\n",
    ") -> np.ndarray:\n",
    "    assert num_crop_points in [21, 42, 63]\n",
    "    neutral_hand_pose = SingleHandPose(\n",
    "        joint_angles=neutral_joint_angles(hand_model).numpy(),\n",
    "        wrist_xform=gt_hand_pose.wrist_xform,\n",
    "    )\n",
    "    open_hand_pose = SingleHandPose(\n",
    "        joint_angles=np.zeros(NUM_JOINTS_PER_HAND, dtype=np.float32),\n",
    "        wrist_xform=gt_hand_pose.wrist_xform,\n",
    "    )\n",
    "\n",
    "    crop_points = []\n",
    "    crop_points.append(landmarks_from_hand_pose(hand_model, gt_hand_pose, hand_idx))\n",
    "    if num_crop_points > 21:\n",
    "        crop_points.append(\n",
    "            landmarks_from_hand_pose(hand_model, neutral_hand_pose, hand_idx)\n",
    "        )\n",
    "    if num_crop_points > 42:\n",
    "        crop_points.append(\n",
    "            landmarks_from_hand_pose(hand_model, open_hand_pose, hand_idx)\n",
    "        )\n",
    "    return np.concatenate(crop_points, axis=0)\n",
    "\n",
    "\n",
    "def gen_crop_cameras_from_pose(\n",
    "    cameras: List[camera.CameraModel],\n",
    "    camera_angles: List[float],\n",
    "    hand_model: HandModel,\n",
    "    hand_pose: SingleHandPose,\n",
    "    hand_idx: int,\n",
    "    num_crop_points: int,\n",
    "    new_image_size: Tuple[int, int],\n",
    "    max_view_num: Optional[int] = None,\n",
    "    sort_camera_index: bool = False,\n",
    "    focal_multiplier: float = 0.95,\n",
    "    mirror_right_hand: bool = True,\n",
    "    min_required_vis_landmarks: int = 19,\n",
    ") -> Dict[int, camera.PinholePlaneCameraModel]:\n",
    "    \n",
    "    \n",
    "    crop_cameras: Dict[int, camera.PinholePlaneCameraModel] = {}\n",
    "    crop_points = _get_crop_points_from_hand_pose(\n",
    "        hand_model,\n",
    "        hand_pose,\n",
    "        hand_idx,\n",
    "        num_crop_points,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    print(f\"crop_points\")\n",
    "    print(crop_points.shape)\n",
    "    \n",
    "    cam_indices = rank_hand_visibility_in_cameras(\n",
    "        cameras=cameras,\n",
    "        hand_model=hand_model,\n",
    "        hand_pose=hand_pose,\n",
    "        hand_idx=hand_idx,\n",
    "        min_required_vis_landmarks=min_required_vis_landmarks,\n",
    "    )\n",
    "\n",
    "    if sort_camera_index:\n",
    "        cam_indices = sorted(cam_indices)\n",
    "\n",
    "    for cam_idx in cam_indices:\n",
    "        crop_cameras[cam_idx] = gen_crop_parameters_from_points(\n",
    "            cameras[cam_idx],\n",
    "            crop_points,\n",
    "            new_image_size,\n",
    "            mirror_img_x=(mirror_right_hand and hand_idx == 1),\n",
    "            camera_angle=camera_angles[cam_idx],\n",
    "            focal_multiplier=focal_multiplier,\n",
    "        )\n",
    "        if len(crop_cameras) == max_view_num:\n",
    "            break\n",
    "\n",
    "    return crop_cameras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import from tracker.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracker.py\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import cv2\n",
    "\n",
    "import lib.common.camera as camera\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "from lib.common.hand import HandModel, NUM_HANDS, scaled_hand_model\n",
    "\n",
    "from lib.data_utils import bundles\n",
    "\n",
    "from lib.models.regressor import RegressorOutput\n",
    "from lib.models.umetrack_model import InputFrameData, InputFrameDesc, InputSkeletonData\n",
    "\n",
    "\n",
    "MM_TO_M = 0.001\n",
    "M_TO_MM = 1000.0\n",
    "MIN_OBSERVED_LANDMARKS = 21\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "MAX_VIEW_NUM = 2\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ViewData:\n",
    "    image: np.ndarray\n",
    "    camera: camera.CameraModel\n",
    "    camera_angle: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InputFrame:\n",
    "    views: List[ViewData]\n",
    "\n",
    "@dataclass\n",
    "class HandTrackerOpts:\n",
    "    num_crop_points: int = 63\n",
    "    enable_memory: bool = True\n",
    "    use_stored_pose_for_crop: bool = True\n",
    "    hand_ratio_in_crop: float = 0.95\n",
    "    min_required_vis_landmarks: int = 19\n",
    "\n",
    "def _warp_image(\n",
    "    src_camera: camera.CameraModel,\n",
    "    dst_camera: camera.CameraModel,\n",
    "    src_image: np.ndarray,\n",
    "    interpolation: int = cv2.INTER_LINEAR,\n",
    "    depth_check: bool = True,\n",
    ") -> np.ndarray:\n",
    "    W, H = dst_camera.width, dst_camera.height\n",
    "    px, py = np.meshgrid(np.arange(W), np.arange(H))\n",
    "    dst_win_pts = np.column_stack((px.flatten(), py.flatten()))\n",
    "\n",
    "    dst_eye_pts = dst_camera.window_to_eye(dst_win_pts)\n",
    "    world_pts = dst_camera.eye_to_world(dst_eye_pts)\n",
    "    src_eye_pts = src_camera.world_to_eye(world_pts)\n",
    "    src_win_pts = src_camera.eye_to_window(src_eye_pts)\n",
    "\n",
    "    # Mask out points with negative z coordinates\n",
    "    if depth_check:\n",
    "        mask = src_eye_pts[:, 2] < 0\n",
    "        src_win_pts[mask] = -1\n",
    "\n",
    "    src_win_pts = src_win_pts.astype(np.float32)\n",
    "\n",
    "    map_x = src_win_pts[:, 0].reshape((H, W))\n",
    "    map_y = src_win_pts[:, 1].reshape((H, W))\n",
    "\n",
    "    return cv2.remap(src_image, map_x, map_y, interpolation)\n",
    "\n",
    "class HandTracker:\n",
    "    def __init__(self, model, opts: HandTrackerOpts) -> None:\n",
    "        self._device: str = \"cuda\" if torch.cuda.device_count() else \"cpu\"\n",
    "        \n",
    "        self._model = model\n",
    "        self._model.to(self._device)\n",
    "\n",
    "        self._input_size = np.array(self._model.getInputImageSizes())\n",
    "        self._num_crop_points = opts.num_crop_points\n",
    "        self._enable_memory = opts.enable_memory\n",
    "        self._hand_ratio_in_crop: float = opts.hand_ratio_in_crop\n",
    "        self._min_required_vis_landmarks: int = opts.min_required_vis_landmarks\n",
    "        self._valid_tracking_history = np.zeros(2, dtype=bool)\n",
    "\n",
    "    def reset_history(self) -> None:\n",
    "        self._valid_tracking_history[:] = False\n",
    "\n",
    "    def gen_crop_cameras(\n",
    "        self,\n",
    "        cameras: List[camera.CameraModel],\n",
    "        camera_angles: List[float],\n",
    "        hand_model: HandModel,\n",
    "        gt_tracking: Dict[int, SingleHandPose],\n",
    "        min_num_crops: int,\n",
    "    ) -> Dict[int, Dict[int, camera.PinholePlaneCameraModel]]:\n",
    "        crop_cameras: Dict[int, Dict[int, camera.PinholePlaneCameraModel]] = {}\n",
    "        if not gt_tracking:\n",
    "            return crop_cameras\n",
    "\n",
    "        for hand_idx, gt_hand_pose in gt_tracking.items():\n",
    "            if gt_hand_pose.hand_confidence < CONFIDENCE_THRESHOLD:\n",
    "                continue\n",
    "            crop_cameras[hand_idx] = gen_crop_cameras_from_pose(\n",
    "                cameras,\n",
    "                camera_angles,\n",
    "                hand_model,\n",
    "                gt_hand_pose,\n",
    "                hand_idx,\n",
    "                self._num_crop_points,\n",
    "                self._input_size,\n",
    "                max_view_num=MAX_VIEW_NUM,\n",
    "                sort_camera_index=True,\n",
    "                focal_multiplier=self._hand_ratio_in_crop,\n",
    "                mirror_right_hand=True,\n",
    "                min_required_vis_landmarks=self._min_required_vis_landmarks,\n",
    "            )\n",
    "\n",
    "        # Remove empty crop_cameras\n",
    "        del_list = []\n",
    "        for hand_idx, per_hand_crop_cameras in crop_cameras.items():\n",
    "            if not per_hand_crop_cameras or len(per_hand_crop_cameras) < min_num_crops:\n",
    "                del_list.append(hand_idx)\n",
    "        for hand_idx in del_list:\n",
    "            del crop_cameras[hand_idx]\n",
    "\n",
    "        return crop_cameras\n",
    "\n",
    "    def track_frame(\n",
    "        self,\n",
    "        sample: InputFrame,\n",
    "        hand_model: HandModel,\n",
    "        crop_cameras: Dict[int, Dict[int, camera.PinholePlaneCameraModel]],\n",
    "    ) -> TrackingResult:\n",
    "        if not crop_cameras:\n",
    "            # Frame without hands\n",
    "            self.reset_history()\n",
    "            return TrackingResult()\n",
    "\n",
    "        frame_data, frame_desc, skeleton_data = self._make_inputs(\n",
    "            sample, hand_model, crop_cameras\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            regressor_output = bundles.to_device(\n",
    "                self._model.regress_pose_use_skeleton(\n",
    "                    frame_data, frame_desc, skeleton_data\n",
    "                ),\n",
    "                torch.device(\"cpu\"),\n",
    "            )\n",
    "\n",
    "        tracking_result = self._gen_tracking_result(\n",
    "            regressor_output,\n",
    "            frame_desc.hand_idx.cpu().numpy(),\n",
    "            crop_cameras,\n",
    "        )\n",
    "        return tracking_result\n",
    "\n",
    "    def track_frame_and_calibrate_scale(\n",
    "        self,\n",
    "        sample: InputFrame,\n",
    "        crop_cameras: Dict[int, Dict[int, camera.PinholePlaneCameraModel]],\n",
    "    ) -> TrackingResult:\n",
    "        if not crop_cameras:\n",
    "            # Frame without hands\n",
    "            self.reset_history()\n",
    "            return TrackingResult()\n",
    "        frame_data, frame_desc, _ = self._make_inputs(sample, None, crop_cameras)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            regressor_output = bundles.to_device(\n",
    "                self._model.regress_pose_pred_skel_scale(frame_data, frame_desc),\n",
    "                torch.device(\"cpu\"),\n",
    "            )\n",
    "\n",
    "        tracking_result = self._gen_tracking_result(\n",
    "            regressor_output,\n",
    "            frame_desc.hand_idx.cpu().numpy(),\n",
    "            crop_cameras,\n",
    "        )\n",
    "        return tracking_result\n",
    "\n",
    "    def _make_inputs(\n",
    "        self,\n",
    "        sample: InputFrame,\n",
    "        hand_model_mm: Optional[HandModel],\n",
    "        crop_cameras: Dict[int, Dict[int, camera.PinholePlaneCameraModel]],\n",
    "    ):\n",
    "        image_idx = 0\n",
    "        left_images = []\n",
    "        intrinsics = []\n",
    "        extrinsics_xf = []\n",
    "        sample_range_n_hands = []\n",
    "        hand_indices = []\n",
    "        for hand_idx, crop_camera_info in crop_cameras.items():\n",
    "            sample_range_start = image_idx\n",
    "            for cam_idx, crop_camera in crop_camera_info.items():\n",
    "                view_data = sample.views[cam_idx]\n",
    "                crop_image = _warp_image(view_data.camera, crop_camera, view_data.image)\n",
    "                left_images.append(crop_image.astype(np.float32) / 255.0)\n",
    "                intrinsics.append(crop_camera.uv_to_window_matrix())\n",
    "\n",
    "                crop_world_to_eye_xf = np.linalg.inv(crop_camera.camera_to_world_xf)\n",
    "                crop_world_to_eye_xf[:3, 3] *= MM_TO_M\n",
    "                extrinsics_xf.append(crop_world_to_eye_xf)\n",
    "\n",
    "                image_idx += 1\n",
    "\n",
    "            if image_idx > sample_range_start:\n",
    "                hand_indices.append(hand_idx)\n",
    "                sample_range_n_hands.append(np.array([sample_range_start, image_idx]))\n",
    "\n",
    "        hand_indices = np.array(hand_indices)\n",
    "        frame_data = InputFrameData(\n",
    "            left_images=torch.from_numpy(np.stack(left_images)).float(),\n",
    "            intrinsics=torch.from_numpy(np.stack(intrinsics)).float(),\n",
    "            extrinsics_xf=torch.from_numpy(np.stack(extrinsics_xf)).float(),\n",
    "        )\n",
    "        frame_desc = InputFrameDesc(\n",
    "            sample_range=torch.from_numpy(np.stack(sample_range_n_hands)).long(),\n",
    "            memory_idx=torch.from_numpy(hand_indices).long(),\n",
    "            # use memory if the hand is previously valid\n",
    "            use_memory=torch.from_numpy(\n",
    "                self._valid_tracking_history[hand_indices]\n",
    "            ).bool(),\n",
    "            hand_idx=torch.from_numpy(hand_indices).long(),\n",
    "        )\n",
    "        skeleton_data = None\n",
    "        if hand_model_mm is not None:\n",
    "            # m -> mm\n",
    "            hand_model_m = scaled_hand_model(hand_model_mm, MM_TO_M)\n",
    "            skeleton_data = InputSkeletonData(\n",
    "                joint_rotation_axes=hand_model_m.joint_rotation_axes.float(),\n",
    "                joint_rest_positions=hand_model_m.joint_rest_positions.float(),\n",
    "            )\n",
    "        return bundles.to_device((frame_data, frame_desc, skeleton_data), self._device)\n",
    "\n",
    "    def _gen_tracking_result(\n",
    "        self,\n",
    "        regressor_output: RegressorOutput,\n",
    "        hand_indices: np.ndarray,\n",
    "        crop_cameras: Dict[int, Dict[int, camera.PinholePlaneCameraModel]],\n",
    "    ) -> TrackingResult:\n",
    "\n",
    "        output_joint_angles = regressor_output.joint_angles.to(\"cpu\").numpy()\n",
    "        output_wrist_xforms = regressor_output.wrist_xfs.to(\"cpu\").numpy()\n",
    "        output_wrist_xforms[..., :3, 3] *= M_TO_MM\n",
    "        output_scales = None\n",
    "        if regressor_output.skel_scales is not None:\n",
    "            output_scales = regressor_output.skel_scales.to(\"cpu\").numpy()\n",
    "\n",
    "        hand_poses = {}\n",
    "        num_views = {}\n",
    "        predicted_scales = {}\n",
    "\n",
    "        for output_idx, hand_idx in enumerate(hand_indices):\n",
    "            raw_handpose = SingleHandPose(\n",
    "                joint_angles=output_joint_angles[output_idx],\n",
    "                wrist_xform=output_wrist_xforms[output_idx],\n",
    "                hand_confidence=1.0,\n",
    "            )\n",
    "            hand_poses[hand_idx] = raw_handpose\n",
    "            num_views[hand_idx] = len(crop_cameras[hand_idx])\n",
    "            if output_scales is not None:\n",
    "                predicted_scales[hand_idx] = output_scales[output_idx]\n",
    "\n",
    "        for hand_idx in range(NUM_HANDS):\n",
    "            hand_valid = False\n",
    "            if hand_idx in hand_poses:\n",
    "                self._valid_tracking_history[hand_idx] = True\n",
    "                hand_valid = True\n",
    "            if hand_valid:\n",
    "                continue\n",
    "            self._valid_tracking_history[hand_idx] = False\n",
    "\n",
    "        return TrackingResult(\n",
    "            hand_poses=hand_poses,\n",
    "            num_views=num_views,\n",
    "            predicted_scales=predicted_scales,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/opt/anaconda3/envs/absolute_track/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/anaconda3/envs/absolute_track/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/anaconda3/envs/absolute_track/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/anaconda3/envs/absolute_track/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/anaconda3/envs/absolute_track/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/kr/x2pj3kwx2tl018qb_jjdfj6m0000gn/T/ipykernel_82750/2607576895.py\", line 9, in <module>\n",
      "    model = load_pretrained_model(model_path)\n",
      "  File \"/Volumes/HJP/KUAICVLab/Hand/AbsoluteTrack/lib/models/model_loader.py\", line 56, in load_pretrained_model\n",
      "    feature_extractor = fe.FeatureExtractor((96, 96), ModelOpts())\n",
      "  File \"/Volumes/HJP/KUAICVLab/Hand/AbsoluteTrack/lib/models/feature_extractor.py\", line 36, in __init__\n",
      "    self._image_backbone, backbone_outshape = model_utils.create_backbone(\n",
      "  File \"/Volumes/HJP/KUAICVLab/Hand/AbsoluteTrack/lib/models/model_utils.py\", line 120, in create_backbone\n",
      "    nn.Conv2d(1, start_planes, kernel_size=3, padding=1),\n",
      "  File \"/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/torch/nn/modules/conv.py\", line 450, in __init__\n",
      "    super().__init__(\n",
      "  File \"/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/torch/nn/modules/conv.py\", line 137, in __init__\n",
      "    self.weight = Parameter(torch.empty(\n",
      "/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/torch/nn/modules/conv.py:137: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  self.weight = Parameter(torch.empty(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretrained_weights.torch\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretrained_models\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_name)\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     11\u001b[0m tracker \u001b[38;5;241m=\u001b[39m HandTracker(model, HandTrackerOpts())\n",
      "File \u001b[0;32m/Volumes/HJP/KUAICVLab/Hand/AbsoluteTrack/lib/models/model_loader.py:64\u001b[0m, in \u001b[0;36mload_pretrained_model\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m     57\u001b[0m temporal \u001b[38;5;241m=\u001b[39m tem\u001b[38;5;241m.\u001b[39mcreate_temporal_model(\n\u001b[1;32m     58\u001b[0m     model_opts,\n\u001b[1;32m     59\u001b[0m     feature_extractor\u001b[38;5;241m.\u001b[39moutput_feature_sizes,\n\u001b[1;32m     60\u001b[0m )\n\u001b[1;32m     61\u001b[0m skeleton_encoder \u001b[38;5;241m=\u001b[39m se\u001b[38;5;241m.\u001b[39mSkeletonEncoder(\n\u001b[1;32m     62\u001b[0m     [model_opts\u001b[38;5;241m.\u001b[39mnSkeletonFeatureChannels, \u001b[38;5;241m*\u001b[39mfeature_extractor\u001b[38;5;241m.\u001b[39moutput_feature_sizes],\n\u001b[1;32m     63\u001b[0m )\n\u001b[0;32m---> 64\u001b[0m regressor_k \u001b[38;5;241m=\u001b[39m \u001b[43m_create_regressor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_opts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_feature_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_skel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredict_skel_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m regressor_u \u001b[38;5;241m=\u001b[39m _create_regressor(\n\u001b[1;32m     71\u001b[0m     model_opts,\n\u001b[1;32m     72\u001b[0m     feature_extractor\u001b[38;5;241m.\u001b[39moutput_feature_sizes,\n\u001b[1;32m     73\u001b[0m     use_skel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     74\u001b[0m     predict_skel_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     75\u001b[0m )\n\u001b[1;32m     77\u001b[0m umetrack_model \u001b[38;5;241m=\u001b[39m UmeTrackModel(\n\u001b[1;32m     78\u001b[0m     feature_extractor\u001b[38;5;241m=\u001b[39mfeature_extractor,\n\u001b[1;32m     79\u001b[0m     temporal\u001b[38;5;241m=\u001b[39mtemporal,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m     regressor_u\u001b[38;5;241m=\u001b[39mregressor_u,\n\u001b[1;32m     83\u001b[0m )\n",
      "File \u001b[0;32m/Volumes/HJP/KUAICVLab/Hand/AbsoluteTrack/lib/models/model_loader.py:40\u001b[0m, in \u001b[0;36m_create_regressor\u001b[0;34m(model_opts, feature_sizes, use_skel, predict_skel_scale)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m model_opts\u001b[38;5;241m.\u001b[39mnSkeletonFeatureChannels \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     39\u001b[0m n_in \u001b[38;5;241m=\u001b[39m _get_n_input_channels(model_opts, use_skel\u001b[38;5;241m=\u001b[39muse_skel)\n\u001b[0;32m---> 40\u001b[0m reg_out_indices, n_out \u001b[38;5;241m=\u001b[39m \u001b[43mreg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_output_index_ranges\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_opts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict_skel_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredict_skel_scale\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reg\u001b[38;5;241m.\u001b[39mPoseRegressor(\n\u001b[1;32m     44\u001b[0m     n_channels_in\u001b[38;5;241m=\u001b[39mn_in,\n\u001b[1;32m     45\u001b[0m     n_output_dims\u001b[38;5;241m=\u001b[39mn_out,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m     feature_map_sizes\u001b[38;5;241m=\u001b[39mfeature_sizes,\n\u001b[1;32m     50\u001b[0m )\n",
      "File \u001b[0;32m/Volumes/HJP/KUAICVLab/Hand/AbsoluteTrack/lib/models/regressor.py:54\u001b[0m, in \u001b[0;36mget_output_index_ranges\u001b[0;34m(mo, predict_skel_scale)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_output_index_ranges\u001b[39m(\n\u001b[1;32m     51\u001b[0m     mo: ModelOpts,\n\u001b[1;32m     52\u001b[0m     predict_skel_scale: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m     53\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Dict[\u001b[38;5;28mstr\u001b[39m, Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]], \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m---> 54\u001b[0m     rigid_samples \u001b[38;5;241m=\u001b[39m \u001b[43m_gen_rigid_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m mo\u001b[38;5;241m.\u001b[39mnWristRigidPts \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(rigid_samples), (\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax supported n_wrist_rigid_pts is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(rigid_samples)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmo\u001b[38;5;241m.\u001b[39mnWristRigidPts\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m     )\n\u001b[1;32m     61\u001b[0m     output_dims \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoint_angles\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrist_xfs\u001b[39m\u001b[38;5;124m\"\u001b[39m: mo\u001b[38;5;241m.\u001b[39mnWristRigidPts \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskel_scales\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m predict_skel_scale \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlandmark_uncertainty_sigmas\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m21\u001b[39m,\n\u001b[1;32m     66\u001b[0m     }\n",
      "File \u001b[0;32m/Volumes/HJP/KUAICVLab/Hand/AbsoluteTrack/lib/models/regressor.py:45\u001b[0m, in \u001b[0;36m_gen_rigid_features\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         rigid_samples_rescaled[i] \u001b[38;5;241m=\u001b[39m rigid_samples[i] \u001b[38;5;241m/\u001b[39m norm \u001b[38;5;241m*\u001b[39m expected_norm\n\u001b[0;32m---> 45\u001b[0m rigid_samples_rescaled \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrigid_samples_rescaled\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rigid_samples_rescaled\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "logging.basicConfig(level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "model_name = \"pretrained_weights.torch\"\n",
    "model_path = os.path.join(\".\", \"pretrained_models\", model_name)\n",
    "model = load_pretrained_model(model_path)\n",
    "model.eval()\n",
    "tracker = HandTracker(model, HandTrackerOpts())\n",
    "\n",
    "DATA_PATH = \"sample_data/recording_00.mp4\"\n",
    "image_pose_stream = SyncedImagePoseStream(DATA_PATH)\n",
    "hand_model = image_pose_stream._hand_pose_labels.hand_model\n",
    "\n",
    "image_pose_list = []\n",
    "for (input_frame, gt_tracking) in image_pose_stream:\n",
    "    image_pose_list.append((input_frame, gt_tracking))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crop_points\n",
      "(63, 3)\n",
      "crop_points\n",
      "(63, 3)\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "\n",
    "input_frame, gt_tracking = image_pose_list[idx]\n",
    "crop_cameras = tracker.gen_crop_cameras(\n",
    "    [view.camera for view in input_frame.views],\n",
    "    image_pose_stream._hand_pose_labels.camera_angles,\n",
    "    hand_model,\n",
    "    gt_tracking,\n",
    "    min_num_crops=1,\n",
    ")\n",
    "#res = tracker.track_frame(input_frame, hand_model, crop_cameras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x3714b37c0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _draw_all_if_interactive at 0x13f995160> (for post_execute):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "object __array__ method not producing an array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/matplotlib/pyplot.py:268\u001b[0m, in \u001b[0;36m_draw_all_if_interactive\u001b[0;34m()\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_draw_all_if_interactive\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m matplotlib\u001b[38;5;241m.\u001b[39mis_interactive():\n\u001b[0;32m--> 268\u001b[0m         \u001b[43mdraw_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/matplotlib/_pylab_helpers.py:131\u001b[0m, in \u001b[0;36mGcf.draw_all\u001b[0;34m(cls, force)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_all_fig_managers():\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force \u001b[38;5;129;01mor\u001b[39;00m manager\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mstale:\n\u001b[0;32m--> 131\u001b[0m         \u001b[43mmanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/matplotlib/backend_bases.py:1905\u001b[0m, in \u001b[0;36mFigureCanvasBase.draw_idle\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_idle_drawing:\n\u001b[1;32m   1904\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_idle_draw_cntx():\n\u001b[0;32m-> 1905\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:387\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;66;03m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\u001b[38;5;241m.\u001b[39m_wait_cursor_for_draw_cm() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\n\u001b[1;32m    386\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m nullcontext()):\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;66;03m# A GUI class may be need to update a window using this draw, so\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;66;03m# don't forget to call the superclass.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdraw()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/matplotlib/artist.py:95\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_wrapper\u001b[39m(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 95\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m renderer\u001b[38;5;241m.\u001b[39m_rasterizing:\n\u001b[1;32m     97\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/matplotlib/figure.py:3161\u001b[0m, in \u001b[0;36mFigure.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3158\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   3159\u001b[0m         \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[0;32m-> 3161\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3162\u001b[0m mimage\u001b[38;5;241m.\u001b[39m_draw_list_compositing_images(\n\u001b[1;32m   3163\u001b[0m     renderer, \u001b[38;5;28mself\u001b[39m, artists, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppressComposite)\n\u001b[1;32m   3165\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfigure\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/matplotlib/patches.py:632\u001b[0m, in \u001b[0;36mPatch.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    630\u001b[0m tpath \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mtransform_path_non_affine(path)\n\u001b[1;32m    631\u001b[0m affine \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mget_affine()\n\u001b[0;32m--> 632\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_paths_with_artist_properties\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maffine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Work around a bug in the PDF and SVG renderers, which\u001b[39;49;00m\n\u001b[1;32m    636\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# do not draw the hatches if the facecolor is fully\u001b[39;49;00m\n\u001b[1;32m    637\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# transparent, but do if it is None.\u001b[39;49;00m\n\u001b[1;32m    638\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_facecolor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_facecolor\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/matplotlib/patches.py:617\u001b[0m, in \u001b[0;36mPatch._draw_paths_with_artist_properties\u001b[0;34m(self, renderer, draw_path_args_list)\u001b[0m\n\u001b[1;32m    614\u001b[0m     renderer \u001b[38;5;241m=\u001b[39m PathEffectRenderer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_path_effects(), renderer)\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m draw_path_args \u001b[38;5;129;01min\u001b[39;00m draw_path_args_list:\n\u001b[0;32m--> 617\u001b[0m     \u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdraw_path_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    619\u001b[0m gc\u001b[38;5;241m.\u001b[39mrestore()\n\u001b[1;32m    620\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:131\u001b[0m, in \u001b[0;36mRendererAgg.draw_path\u001b[0;34m(self, gc, path, transform, rgbFace)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_renderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrgbFace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOverflowError\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m         cant_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: object __array__ method not producing an array"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "object __array__ method not producing an array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/IPython/core/formatters.py:340\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[1;32m    342\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/IPython/core/pylabtools.py:152\u001b[0m, in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[1;32m    150\u001b[0m     FigureCanvasBase(fig)\n\u001b[0;32m--> 152\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/matplotlib/backend_bases.py:2204\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2201\u001b[0m     \u001b[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[1;32m   2202\u001b[0m     \u001b[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[1;32m   2203\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[0;32m-> 2204\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mprint_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2205\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2206\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfacecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfacecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2207\u001b[0m \u001b[43m            \u001b[49m\u001b[43medgecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medgecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2208\u001b[0m \u001b[43m            \u001b[49m\u001b[43morientation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morientation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2209\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbbox_inches_restore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_bbox_inches_restore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2210\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2211\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/matplotlib/backend_bases.py:2054\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m     optional_kws \u001b[38;5;241m=\u001b[39m {  \u001b[38;5;66;03m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[1;32m   2051\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medgecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morientation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2052\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox_inches_restore\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m   2053\u001b[0m     skip \u001b[38;5;241m=\u001b[39m optional_kws \u001b[38;5;241m-\u001b[39m {\u001b[38;5;241m*\u001b[39minspect\u001b[38;5;241m.\u001b[39msignature(meth)\u001b[38;5;241m.\u001b[39mparameters}\n\u001b[0;32m-> 2054\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mwraps(meth)(\u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2056\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[1;32m   2057\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m meth\n",
      "File \u001b[0;32m/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:496\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_png\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, \u001b[38;5;241m*\u001b[39m, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pil_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    450\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;124;03m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;124;03m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 496\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_print_pil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:444\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_print_pil\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, fmt, pil_kwargs, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    440\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;124;03m    Draw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;124;03m    *pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 444\u001b[0m     \u001b[43mFigureCanvasAgg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     mpl\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mimsave(\n\u001b[1;32m    446\u001b[0m         filename_or_obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_rgba(), \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mfmt, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    447\u001b[0m         dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mdpi, metadata\u001b[38;5;241m=\u001b[39mmetadata, pil_kwargs\u001b[38;5;241m=\u001b[39mpil_kwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:387\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;66;03m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\u001b[38;5;241m.\u001b[39m_wait_cursor_for_draw_cm() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\n\u001b[1;32m    386\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m nullcontext()):\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;66;03m# A GUI class may be need to update a window using this draw, so\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;66;03m# don't forget to call the superclass.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdraw()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/matplotlib/artist.py:95\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_wrapper\u001b[39m(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 95\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m renderer\u001b[38;5;241m.\u001b[39m_rasterizing:\n\u001b[1;32m     97\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/matplotlib/figure.py:3161\u001b[0m, in \u001b[0;36mFigure.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3158\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   3159\u001b[0m         \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[0;32m-> 3161\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3162\u001b[0m mimage\u001b[38;5;241m.\u001b[39m_draw_list_compositing_images(\n\u001b[1;32m   3163\u001b[0m     renderer, \u001b[38;5;28mself\u001b[39m, artists, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppressComposite)\n\u001b[1;32m   3165\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfigure\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/matplotlib/patches.py:632\u001b[0m, in \u001b[0;36mPatch.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    630\u001b[0m tpath \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mtransform_path_non_affine(path)\n\u001b[1;32m    631\u001b[0m affine \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mget_affine()\n\u001b[0;32m--> 632\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_paths_with_artist_properties\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maffine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Work around a bug in the PDF and SVG renderers, which\u001b[39;49;00m\n\u001b[1;32m    636\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# do not draw the hatches if the facecolor is fully\u001b[39;49;00m\n\u001b[1;32m    637\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# transparent, but do if it is None.\u001b[39;49;00m\n\u001b[1;32m    638\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_facecolor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_facecolor\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/matplotlib/patches.py:617\u001b[0m, in \u001b[0;36mPatch._draw_paths_with_artist_properties\u001b[0;34m(self, renderer, draw_path_args_list)\u001b[0m\n\u001b[1;32m    614\u001b[0m     renderer \u001b[38;5;241m=\u001b[39m PathEffectRenderer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_path_effects(), renderer)\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m draw_path_args \u001b[38;5;129;01min\u001b[39;00m draw_path_args_list:\n\u001b[0;32m--> 617\u001b[0m     \u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdraw_path_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    619\u001b[0m gc\u001b[38;5;241m.\u001b[39mrestore()\n\u001b[1;32m    620\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/absolute_track/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:131\u001b[0m, in \u001b[0;36mRendererAgg.draw_path\u001b[0;34m(self, gc, path, transform, rgbFace)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_renderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrgbFace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOverflowError\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m         cant_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: object __array__ method not producing an array"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.ones((100, 100, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (480, 636))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(input_frame.views[0].image), input_frame.views[0].image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) :-1: error: (-5:Bad argument) in function 'remap'\n> Overload resolution failed:\n>  - src is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'src'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mtracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhand_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_cameras\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 148\u001b[0m, in \u001b[0;36mHandTracker.track_frame\u001b[0;34m(self, sample, hand_model, crop_cameras)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_history()\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TrackingResult()\n\u001b[0;32m--> 148\u001b[0m frame_data, frame_desc, skeleton_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhand_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_cameras\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    152\u001b[0m     regressor_output \u001b[38;5;241m=\u001b[39m bundles\u001b[38;5;241m.\u001b[39mto_device(\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mregress_pose_use_skeleton(\n\u001b[1;32m    154\u001b[0m             frame_data, frame_desc, skeleton_data\n\u001b[1;32m    155\u001b[0m         ),\n\u001b[1;32m    156\u001b[0m         torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    157\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[22], line 206\u001b[0m, in \u001b[0;36mHandTracker._make_inputs\u001b[0;34m(self, sample, hand_model_mm, crop_cameras)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cam_idx, crop_camera \u001b[38;5;129;01min\u001b[39;00m crop_camera_info\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    205\u001b[0m     view_data \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mviews[cam_idx]\n\u001b[0;32m--> 206\u001b[0m     crop_image \u001b[38;5;241m=\u001b[39m \u001b[43m_warp_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mview_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcamera\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_camera\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mview_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     left_images\u001b[38;5;241m.\u001b[39mappend(crop_image\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m)\n\u001b[1;32m    208\u001b[0m     intrinsics\u001b[38;5;241m.\u001b[39mappend(crop_camera\u001b[38;5;241m.\u001b[39muv_to_window_matrix())\n",
      "Cell \u001b[0;32mIn[22], line 77\u001b[0m, in \u001b[0;36m_warp_image\u001b[0;34m(src_camera, dst_camera, src_image, interpolation, depth_check)\u001b[0m\n\u001b[1;32m     74\u001b[0m map_x \u001b[38;5;241m=\u001b[39m src_win_pts[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape((H, W))\n\u001b[1;32m     75\u001b[0m map_y \u001b[38;5;241m=\u001b[39m src_win_pts[:, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape((H, W))\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremap\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) :-1: error: (-5:Bad argument) in function 'remap'\n> Overload resolution failed:\n>  - src is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'src'\n"
     ]
    }
   ],
   "source": [
    "res = tracker.track_frame(input_frame, hand_model, crop_cameras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umetrack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

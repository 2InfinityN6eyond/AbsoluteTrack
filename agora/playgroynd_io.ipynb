{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from ipywidgets import interactive, IntSlider, Output\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "# %matplotlib widget \n",
    "\n",
    "import mediapipe as mp\n",
    "\n",
    "\n",
    "\n",
    "img = np.zeros((100, 100), dtype=np.uint8)\n",
    "plt.imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import os\n",
    "\n",
    "import av\n",
    "import fnmatch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import lib.data_utils.fs as fs\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# from lib.tracker.perspective_crop import landmarks_from_hand_pose\n",
    "\n",
    "\n",
    "#from multiprocessing import Pool\n",
    "#from typing import Optional, Tuple\n",
    "\n",
    "from lib.models.model_loader import load_pretrained_model\n",
    "\n",
    "#from lib.tracker.tracker import HandTracker, HandTrackerOpts, InputFrame, ViewData\n",
    "from lib.tracker.video_pose_data import SyncedImagePoseStream\n",
    "\n",
    "from typing import Dict, NamedTuple\n",
    "\n",
    "#from lib.common.hand import NUM_HANDS, NUM_LANDMARKS_PER_HAND\n",
    "NUM_HANDS = 2\n",
    "NUM_LANDMARKS_PER_HAND = 21\n",
    "NUM_FINGERTIPS_PER_HAND = 5\n",
    "NUM_JOINTS_PER_HAND = 22\n",
    "LEFT_HAND_INDEX = 0\n",
    "RIGHT_HAND_INDEX = 1\n",
    "\n",
    "NUM_DIGITS: int = 5\n",
    "NUM_JOINT_FRAMES: int = 1 + 1 + 3 * 5  # root + wrist + finger frames * 5\n",
    "DOF_PER_FINGER: int = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import from lib.common.affine.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "\n",
    "def transform3(m, v):\n",
    "    return transform_vec3(m, v) + m[..., :3, 3]\n",
    "\n",
    "\n",
    "def transform_vec3(m, v):\n",
    "    if m.ndim == 2:\n",
    "        return (v.reshape(-1, 3) @ m[:3, :3].T).reshape(v.shape)\n",
    "    else:\n",
    "        return (m[..., :3, :3] @ v[..., None]).squeeze(-1)\n",
    "\n",
    "\n",
    "def normalized(v: np.ndarray, axis: int = -1, eps: float = 5.43e-20) -> np.ndarray:\n",
    "    d = np.maximum(eps, (v * v).sum(axis=axis, keepdims=True) ** 0.5)\n",
    "    return v / d\n",
    "\n",
    "\n",
    "def skew_matrix(v: np.ndarray) -> np.ndarray:\n",
    "    res = np.array(\n",
    "        [[0, -v[2], v[1]], [v[2], 0, -v[0]], [-v[1], v[0], 0]], dtype=v.dtype\n",
    "    )\n",
    "    return res\n",
    "\n",
    "\n",
    "def from_two_vectors(a_orig: np.ndarray, b_orig: np.ndarray) -> np.ndarray:\n",
    "    a = normalized(a_orig)\n",
    "    b = normalized(b_orig)\n",
    "    v = np.cross(a, b)\n",
    "    s = np.linalg.norm(v)\n",
    "    c = np.dot(a, b)\n",
    "    v_mat = skew_matrix(v)\n",
    "\n",
    "    rot = np.eye(3, 3) + v_mat + np.matmul(v_mat, v_mat) * (1 - c) / (max(s * s, 1e-15))\n",
    "\n",
    "    return rot\n",
    "\n",
    "\n",
    "def make_look_at_matrix(\n",
    "    orig_world_to_eye: np.ndarray,\n",
    "    center: np.ndarray,\n",
    "    camera_angle: float = 0,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    args:\n",
    "        orig_world_to_eye:  world to eye transform\n",
    "            inverse of camera.camera_to_world_xf\n",
    "        center:  np.array. (3,)\n",
    "            3D world coordinate of center of the object of interest\n",
    "        camera_angle: the angle of the camera\n",
    "            camera_angle: how the camera is oriented physically so that we can rotate the object of\n",
    "            interest to the 'upright' direction\n",
    "    \"\"\"\n",
    "    center_local = transform3(orig_world_to_eye, center)\n",
    "    z_dir_local = center_local / np.linalg.norm(center_local)\n",
    "    delta_r_local = from_two_vectors(\n",
    "        np.array([0, 0, 1], dtype=center.dtype), z_dir_local\n",
    "    )\n",
    "    orig_eye_to_world = np.linalg.inv(orig_world_to_eye)\n",
    "\n",
    "    new_eye_to_world = orig_eye_to_world.copy()\n",
    "    new_eye_to_world[0:3, 0:3] = orig_eye_to_world[0:3, 0:3] @ delta_r_local\n",
    "\n",
    "    # Locally rotate the z axis to align with the camera angle\n",
    "    z_local_rot = Rotation.from_euler(\"z\", camera_angle, degrees=True).as_matrix()\n",
    "    new_eye_to_world[0:3, 0:3] = new_eye_to_world[0:3, 0:3] @ z_local_rot\n",
    "\n",
    "    return np.linalg.inv(new_eye_to_world)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import from lib.common.camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import abc\n",
    "import json\n",
    "import math\n",
    "from typing import NamedTuple, Sequence, Tuple, Type\n",
    "\n",
    "import numpy as np\n",
    "from typing_extensions import Protocol, runtime_checkable\n",
    "\n",
    "\n",
    "class CameraProjection(Protocol):\n",
    "    \"\"\"\n",
    "    Defines a projection from a 3D `xyz` direction or point to 2D.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    @abc.abstractmethod\n",
    "    def project(cls, v):\n",
    "        \"\"\"\n",
    "        Project a 3d vector in eye space down to 2d.\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    @classmethod\n",
    "    @abc.abstractmethod\n",
    "    def unproject(cls, p):\n",
    "        \"\"\"\n",
    "        Unproject a 2d point to a unit-length vector in eye space.\n",
    "\n",
    "        `project(unproject(p)) == p`\n",
    "        `unproject(project(v)) == v / |v|`\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "\n",
    "@runtime_checkable\n",
    "class DistortionModel(Protocol):\n",
    "    @abc.abstractmethod\n",
    "    def evaluate(self: Sequence[float], p: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        p: ndarray[..., 2]\n",
    "            Array of 2D points, of arbitrary batch shape.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        q: ndarray[..., 2]\n",
    "            Distorted points with same shape as input\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "\n",
    "class PerspectiveProjection(CameraProjection):\n",
    "    @staticmethod\n",
    "    def project(v):\n",
    "        # map to [x/z, y/z]\n",
    "        assert v.shape[-1] == 3\n",
    "        return v[..., :2] / v[..., 2, None]\n",
    "\n",
    "    @staticmethod\n",
    "    def unproject(p):\n",
    "        # map to [u,v,1] and renormalize\n",
    "        assert p.shape[-1] == 2\n",
    "        x, y = np.moveaxis(p, -1, 0)\n",
    "        v = np.stack((x, y, np.ones(shape=x.shape, dtype=x.dtype)), axis=-1)\n",
    "        v = normalized(v, axis=-1)\n",
    "        return v\n",
    "\n",
    "\n",
    "class ArctanProjection(CameraProjection):\n",
    "    @staticmethod\n",
    "    def project(p, eps: float = 2.0**-128):\n",
    "        \n",
    "        \"\"\"\n",
    "        eye space to projected space\n",
    "        It seems arctan projection is same as equid\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        assert p.shape[-1] == 3\n",
    "        x, y, z = np.moveaxis(p, -1, 0)\n",
    "        r = np.sqrt(x * x + y * y)\n",
    "        s = np.arctan2(r, z) / np.maximum(r, eps)\n",
    "        return np.stack((x * s, y * s), axis=-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def unproject(uv):\n",
    "        assert uv.shape[-1] == 2\n",
    "        u, v = np.moveaxis(uv, -1, 0)\n",
    "        r = np.sqrt(u * u + v * v)\n",
    "        c = np.cos(r)\n",
    "        s = np.sinc(r / np.pi)\n",
    "        return np.stack([u * s, v * s, c], axis=-1)\n",
    "\n",
    "\n",
    "class NoDistortion(NamedTuple):\n",
    "    \"\"\"\n",
    "    A trivial distortion model that does not distort the incoming rays.\n",
    "    \"\"\"\n",
    "\n",
    "    def evaluate(self, p: np.ndarray) -> np.ndarray:\n",
    "        return p\n",
    "\n",
    "\n",
    "class Fisheye62CameraModel(NamedTuple):\n",
    "    \"\"\"\n",
    "    Fisheye62CameraModel model, with 6 radial and 2 tangential coeffs.\n",
    "    \"\"\"\n",
    "\n",
    "    k1: float\n",
    "    k2: float\n",
    "    k3: float\n",
    "    k4: float\n",
    "    p1: float\n",
    "    p2: float\n",
    "    k5: float\n",
    "    k6: float\n",
    "\n",
    "    def evaluate(self: Sequence[float], p: np.ndarray) -> np.ndarray:\n",
    "        k1, k2, k3, k4, p1, p2, k5, k6 = self\n",
    "        # radial component\n",
    "        r2 = (p * p).sum(axis=-1, keepdims=True)\n",
    "        r2 = np.clip(r2, -np.pi**2, np.pi**2)\n",
    "        r4 = r2 * r2\n",
    "        r6 = r2 * r4\n",
    "        r8 = r4 * r4\n",
    "        r10 = r4 * r6\n",
    "        r12 = r6 * r6\n",
    "        radial = 1 + k1 * r2 + k2 * r4 + k3 * r6 + k4 * r8 + k5 * r10 + k6 * r12\n",
    "        uv = p * radial\n",
    "\n",
    "        # tangential component\n",
    "        x, y = uv[..., 0], uv[..., 1]\n",
    "        x2 = x * x\n",
    "        y2 = y * y\n",
    "        xy = x * y\n",
    "        r2 = x2 + y2\n",
    "        x += 2 * p2 * xy + p1 * (r2 + 2 * x2)\n",
    "        y += 2 * p1 * xy + p2 * (r2 + 2 * y2)\n",
    "        return np.stack((x, y), axis=-1)\n",
    "\n",
    "\n",
    "    def undistort(self, q: np.ndarray, solver_iters: int = 50) -> np.ndarray :\n",
    "        \"\"\"\n",
    "        Undistorts 2D points using the fisheye radial distortion parameters.\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "        distorted_points : ndarray[..., 2]\n",
    "            Array of distorted 2D points to be undistorted.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "\n",
    "        undistorted_points : ndarray[..., 2]\n",
    "            Array of undistorted 2D points.\n",
    "        \"\"\"\n",
    "        k1, k2, k3, k4, p1, p2, k5, k6 = self\n",
    "        x_distorted = q[..., 0]\n",
    "        y_distorted = q[..., 1]\n",
    "\n",
    "        # Initial guess: the distorted points as undistorted\n",
    "        x_undistorted = x_distorted.copy()\n",
    "        y_undistorted = y_distorted.copy()\n",
    "\n",
    "        for _ in range(5):  # Perform 5 iterations to refine the undistorted coordinates\n",
    "            r2 = x_undistorted**2 + y_undistorted**2\n",
    "            radial_factor = (\n",
    "                1 + k1*r2 + k2 * r2**2 + \n",
    "                k3 * r2**3 + k4 * r2**4 +\n",
    "                k5 * r2**5 + k6 * r2**6\n",
    "            )\n",
    "\n",
    "            # Update the undistorted points by scaling the distorted points\n",
    "            x_undistorted = x_distorted / radial_factor\n",
    "            y_undistorted = y_distorted / radial_factor\n",
    "\n",
    "        return np.stack([x_undistorted, y_undistorted], axis=-1)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# API Conventions and naming\n",
    "#\n",
    "# Points have the xyz or uv components in the last axis, and may have\n",
    "# arbitrary batch shapes. ([...,2] for 2d and [...,3] for 3d).\n",
    "#\n",
    "# v\n",
    "#    3D xyz position in eye space, usually unit-length.\n",
    "# p\n",
    "#    projected uv coordinates: `p = project(v)`\n",
    "# q\n",
    "#    distorted uv coordinates: `q = distort(p)`\n",
    "# w\n",
    "#    window coordinates: `q = q * f + [cx, cy]`\n",
    "#\n",
    "# A trailing underscore (e.g. `p_`, `q_`) should be read as \"hat\", and\n",
    "# generally indicates an approximation to another value.\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class CameraModel(CameraProjection, abc.ABC):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    width, height : int\n",
    "        Size of the sensor window\n",
    "\n",
    "    f : float or tuple(float, float)\n",
    "        Focal length\n",
    "\n",
    "    c : tuple(float, float)\n",
    "        Optical center in window coordinates\n",
    "\n",
    "    distort_coeffs\n",
    "        Forward distortion coefficients (eye -> window).\n",
    "\n",
    "        If this is an instance of DistortionModel, it will be used as-is\n",
    "        (even if it's a different polynomial than this camera model\n",
    "        would normally use.) If it's a simple tuple or array, it will\n",
    "        used as coefficients for `self.distortion_model`.\n",
    "\n",
    "    camera_to_world_xf : np.ndarray\n",
    "        Camera's position and orientation in world space, represented as\n",
    "        a 3x4 or 4x4 matrix.\n",
    "\n",
    "        The matrix be a rigid transform (only rotation and translation).\n",
    "\n",
    "        You can change a camera's camera_to_world_xf after construction by\n",
    "        assigning to or modifying this matrix.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    Most attributes are the same as constructor parameters.\n",
    "\n",
    "    distortion_model\n",
    "        Class attribute giving the distortion model for new instances.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    width: int\n",
    "    height: int\n",
    "\n",
    "    f: Tuple[float, float]\n",
    "    c: Tuple[float, float]\n",
    "\n",
    "    camera_to_world_xf: np.ndarray\n",
    "\n",
    "    distortion_model: Type[DistortionModel]\n",
    "    distort: DistortionModel\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        width,\n",
    "        height,\n",
    "        f,\n",
    "        c,\n",
    "        distort_coeffs,\n",
    "        camera_to_world_xf=None,\n",
    "    ):  # pylint: disable=super-init-not-called (see issue 4790 on pylint github)\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "\n",
    "        # f can be either a scalar or (fx,fy) pair. We only fit scalars,\n",
    "        # but may load (fx, fy) from a stored file.\n",
    "        self.f = tuple(np.broadcast_to(f, 2))\n",
    "        self.c = tuple(c)\n",
    "\n",
    "        if camera_to_world_xf is None:\n",
    "            self.camera_to_world_xf = np.eye(4)\n",
    "        else:\n",
    "            self.camera_to_world_xf = camera_to_world_xf\n",
    "\n",
    "        if isinstance(distort_coeffs, DistortionModel):\n",
    "            self.distort = distort_coeffs\n",
    "        else:\n",
    "            self.distort = self.distortion_model(*distort_coeffs)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"{type(self).__name__}({self.width}x{self.height}, f={self.f} c={self.c}\"\n",
    "        )\n",
    "\n",
    "    def copy(self, camera_to_world_xf=None):\n",
    "        \"\"\"Return a copy of this camera\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        camera_to_world_xf : 4x4 np.ndarray\n",
    "            Optional new camera_to_world_xf for the new camera model.\n",
    "            Default is to copy this camera's camera_to_world_xf.\n",
    "        \"\"\"\n",
    "        return self.crop(0, 0, self.width, self.height, camera_to_world_xf=camera_to_world_xf)\n",
    "\n",
    "    def world_to_eye(self, v):\n",
    "        \"\"\"\n",
    "        Apply camera camera_to_world_xf to points `v` to get eye coords\n",
    "        \"\"\"\n",
    "        return transform_vec3(self.camera_to_world_xf.T, v - self.camera_to_world_xf[:3, 3])\n",
    "\n",
    "    def eye_to_world(self, v):\n",
    "        \"\"\"\n",
    "        Apply inverse camera camera_to_world_xf to eye points `v` to get world coords\n",
    "        \"\"\"\n",
    "        return transform3(self.camera_to_world_xf, v)\n",
    "\n",
    "    def eye_to_window(self, v):\n",
    "        \"\"\"Project eye coordinates to 2d window coordinates\"\"\"\n",
    "        p = self.project(v)\n",
    "        q = self.distort.evaluate(p)\n",
    "        return q * self.f + self.c\n",
    "\n",
    "    def window_to_eye(self, w):\n",
    "        \"\"\"Unproject 2d window coordinates to unit-length 3D eye coordinates\"\"\"\n",
    "        q = (np.asarray(w) - self.c) / self.f\n",
    "        # assert isinstance(\n",
    "        #     self.distort, NoDistortion\n",
    "        # ), \"Only unprojection for NoDistortion camera is supported\"\n",
    "        return self.unproject(q)\n",
    "\n",
    "    def crop(\n",
    "        self,\n",
    "        src_x,\n",
    "        src_y,\n",
    "        target_width,\n",
    "        target_height,\n",
    "        scale=1,\n",
    "        camera_to_world_xf=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Return intrinsics for a crop of the sensor image.\n",
    "\n",
    "        No scaling is applied; this just returns the model for a sub-\n",
    "        array of image data. (Or for a larger array, if (x,y)<=0 and\n",
    "        (width, height) > (self.width, self.height).\n",
    "\n",
    "        To do both cropping and scaling, use :meth:`subrect`\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x, y, width, height\n",
    "            Location and size in this camera's window coordinates\n",
    "        \"\"\"\n",
    "        return type(self)(\n",
    "            target_width,\n",
    "            target_height,\n",
    "            np.asarray(self.f) * scale,\n",
    "            (np.array(self.c) - (src_x, src_y) + 0.5) * scale - 0.5,\n",
    "            self.distort,\n",
    "            self.camera_to_world_xf if camera_to_world_xf is None else camera_to_world_xf,\n",
    "        )\n",
    "\n",
    "\n",
    "# Camera models\n",
    "# =============\n",
    "\n",
    "\n",
    "class PinholePlaneCameraModel(PerspectiveProjection, CameraModel):\n",
    "    distortion_model = NoDistortion\n",
    "\n",
    "    def uv_to_window_matrix(self):\n",
    "        \"\"\"Return the 3x3 intrinsics matrix\"\"\"\n",
    "        return np.array(\n",
    "            [[self.f[0], 0, self.c[0]], [0, self.f[1], self.c[1]], [0, 0, 1]]\n",
    "        )\n",
    "\n",
    "\n",
    "class Fisheye62CameraModel(ArctanProjection, CameraModel):\n",
    "    distortion_model = Fisheye62CameraModel\n",
    "    \n",
    "    @staticmethod\n",
    "    def unproject(uv):\n",
    "        \"\"\"\n",
    "        Unproject 2D window coordinates to unit-length 3D eye coordinates.\n",
    "        \n",
    "        This implementation assumes that `uv` are in normalized image coordinates\n",
    "        (i.e., after subtracting the principal point and dividing by the focal length).\n",
    "        \"\"\"\n",
    "        assert uv.shape[-1] == 2, \"Input should be a 2D point or array of 2D points.\"\n",
    "\n",
    "        u, v = np.moveaxis(uv, -1, 0)\n",
    "        r = np.sqrt(u * u + v * v)\n",
    "\n",
    "        # Compute the angle of the ray in radians\n",
    "        theta = np.arctan(r)\n",
    "\n",
    "        # Compute the normalized z component (cosine of theta)\n",
    "        z = np.cos(theta)\n",
    "\n",
    "        # Compute the radial distance on the image plane\n",
    "        r_normalized = np.sin(theta)\n",
    "\n",
    "        # Normalize u and v by their radial distance to get the x and y components\n",
    "        x = u * r_normalized / np.maximum(r, 1e-12)\n",
    "        y = v * r_normalized / np.maximum(r, 1e-12)\n",
    "\n",
    "        return np.stack([x, y, z], axis=-1)\n",
    "\n",
    "\n",
    "def read_camera_from_json(js):\n",
    "    if isinstance(js, str):\n",
    "        js = json.loads(js)\n",
    "    js = js.get(\"Camera\", js)\n",
    "\n",
    "    width = js[\"ImageSizeX\"]\n",
    "    height = js[\"ImageSizeY\"]\n",
    "    model = js[\"DistortionModel\"]\n",
    "    fx = js[\"fx\"]\n",
    "    fy = js[\"fy\"]\n",
    "    cx = js[\"cx\"]\n",
    "    cy = js[\"cy\"]\n",
    "\n",
    "    if model == \"PinholePlane\":\n",
    "        cls = PinholePlaneCameraModel\n",
    "    elif model == \"FishEye62\":\n",
    "        cls = Fisheye62CameraModel\n",
    "\n",
    "    distort_params = cls.distortion_model._fields\n",
    "    coeffs = [js[name] for name in distort_params]\n",
    "\n",
    "    return cls(width, height, (fx, fy), (cx, cy), coeffs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import from lib.common.crop.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def gen_intrinsics_from_bounding_pts(\n",
    "    pts_eye: np.ndarray, image_w: int, image_h: int, min_focal: float = 5\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    pts_ndc = pts_eye[..., 0:2] / pts_eye[..., 2:]\n",
    "    img_size = np.array([image_w, image_h], dtype=pts_eye.dtype)\n",
    "    # Given our convention, we need to shift one pixel before dividing by 2.\n",
    "    cx_cy = (img_size - 1) / 2\n",
    "    fx_fy = cx_cy / np.absolute(pts_ndc).max()\n",
    "\n",
    "    # Some sanity checks\n",
    "    if np.any(pts_eye[..., 2:] < 0.0001) or np.any(fx_fy < min_focal):\n",
    "        raise ValueError(\"Unable to create crop camera\", fx_fy)\n",
    "\n",
    "    return fx_fy, cx_cy\n",
    "\n",
    "\n",
    "def gen_crop_parameters_from_points(\n",
    "    camera_orig: CameraModel,\n",
    "    pts_world,\n",
    "    new_image_size: Tuple[int, int],\n",
    "    mirror_img_x: bool,\n",
    "    camera_angle: float = 0,\n",
    "    focal_multiplier: float = 0.95,\n",
    ") -> PinholePlaneCameraModel:\n",
    "    \"\"\"\n",
    "    Given the original camera transform and a list of 3D points in the world space,\n",
    "    compute the new perspective camera that makes sure after projection all the points\n",
    "    can be projected inside the image.\n",
    "\n",
    "    Auguments:\n",
    "    * camera_orig: the original camera used for generating an image. The returned camera\n",
    "        will have the same position but different rotation and intrinsics parameters.\n",
    "    * pts_world: points in the world space that must be projected inside the image by\n",
    "        the generated world to eye transform and intrinsics.\n",
    "    * new_image_size: target image size\n",
    "    * mirror_img_x: whether to flip the image. A typical use case is we usually mirror the\n",
    "        right hand images so that a model need to handle left hand data only\n",
    "    * camera_angle: how the camera is oriented physically so that we can rotate the object of\n",
    "        interest to the 'upright' direction\n",
    "    * focal_multiplier: when less than 1, we are zooming out a little. The effect on the image\n",
    "        is some margin will be left at the boundary.\n",
    "    \"\"\"\n",
    "    orig_world_to_eye_xf = np.linalg.inv(camera_orig.camera_to_world_xf)\n",
    "\n",
    "    crop_center = (pts_world.min(axis=0) + pts_world.max(axis=0)) / 2.0\n",
    "    new_world_to_eye = make_look_at_matrix(\n",
    "        orig_world_to_eye_xf, crop_center, camera_angle\n",
    "    )\n",
    "    if mirror_img_x:\n",
    "        mirrorx = np.eye(4, dtype=np.float32)\n",
    "        mirrorx[0, 0] = -1\n",
    "        new_world_to_eye = mirrorx @ new_world_to_eye\n",
    "\n",
    "    fx_fy, cx_cy = gen_intrinsics_from_bounding_pts(\n",
    "        transform3(new_world_to_eye, pts_world),\n",
    "        new_image_size[0],\n",
    "        new_image_size[1],\n",
    "    )\n",
    "    fx_fy = focal_multiplier * fx_fy\n",
    "\n",
    "    return PinholePlaneCameraModel(\n",
    "        width=new_image_size[0],\n",
    "        height=new_image_size[1],\n",
    "        f=fx_fy,\n",
    "        c=cx_cy,\n",
    "        distort_coeffs=[],\n",
    "        camera_to_world_xf=np.linalg.inv(new_world_to_eye),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import from lib.tracker.tracking_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, NamedTuple\n",
    "\n",
    "class SingleHandPose(NamedTuple):\n",
    "    \"\"\"\n",
    "    A hand pose is composed of two fields:\n",
    "    1) joint angles where # joints == # DoFs\n",
    "    2) root-to-world rigid wrist transformation\n",
    "    \"\"\"\n",
    "\n",
    "    joint_angles: np.ndarray = np.zeros(NUM_JOINTS_PER_HAND, dtype=np.float32)\n",
    "    wrist_xform: np.ndarray = np.eye(4, dtype=np.float32)\n",
    "    hand_confidence: float = 1.0\n",
    "\n",
    "\n",
    "# Tracking result maps from hand_index to hand_pose\n",
    "class TrackingResult(NamedTuple):\n",
    "    hand_poses: Dict[int, SingleHandPose] = {}\n",
    "    num_views: Dict[int, int] = {}\n",
    "    predicted_scales: Dict[int, float] = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import from lib.common.crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Tuple\n",
    "\n",
    "# import lib.common.camera as camera\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# from . import affine\n",
    "\n",
    "def gen_intrinsics_from_bounding_pts(\n",
    "    pts_eye: np.ndarray, image_w: int, image_h: int, min_focal: float = 5\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    pts_ndc = pts_eye[..., 0:2] / pts_eye[..., 2:]\n",
    "    img_size = np.array([image_w, image_h], dtype=pts_eye.dtype)\n",
    "    # Given our convention, we need to shift one pixel before dividing by 2.\n",
    "    cx_cy = (img_size - 1) / 2\n",
    "    fx_fy = cx_cy / np.absolute(pts_ndc).max()\n",
    "\n",
    "    # Some sanity checks\n",
    "    if np.any(pts_eye[..., 2:] < 0.0001) or np.any(fx_fy < min_focal):\n",
    "        raise ValueError(\"Unable to create crop camera\", fx_fy)\n",
    "\n",
    "    return fx_fy, cx_cy\n",
    "\n",
    "\n",
    "def gen_crop_parameters_from_points(\n",
    "    camera_orig: CameraModel,\n",
    "    pts_world,\n",
    "    new_image_size: Tuple[int, int],\n",
    "    mirror_img_x: bool,\n",
    "    camera_angle: float = 0,\n",
    "    focal_multiplier: float = 0.95,\n",
    ") -> PinholePlaneCameraModel:\n",
    "    \"\"\"\n",
    "    Given the original camera transform and a list of 3D points in the world space,\n",
    "    compute the new perspective camera that makes sure after projection all the points\n",
    "    can be projected inside the image.\n",
    "\n",
    "    Auguments:\n",
    "    * camera_orig: the original camera used for generating an image. The returned camera\n",
    "        will have the same position but differet rotation and intrinsics parameters.\n",
    "    * pts_world: np.array. (21, 3) or (42, 3) or (63, 3)\n",
    "        points in the world space that must be projected inside the image by\n",
    "        the generated world to eye transform and intrinsics.\n",
    "    * new_image_size: target image size\n",
    "    * mirror_img_x: whether to flip the image. A typical use case is we usually mirror the\n",
    "        right hand images so that a model need to handle left hand data only\n",
    "    * camera_angle: how the camera is oriented physically so that we can rotate the object of\n",
    "        interest to the 'upright' direction\n",
    "    * focal_multiplier: when less than 1, we are zooming out a little. The effect on the image\n",
    "        is some margin will be left at the boundary.\n",
    "    \"\"\"\n",
    "    orig_world_to_eye_xf = np.linalg.inv(camera_orig.camera_to_world_xf)\n",
    "\n",
    "    crop_center = (pts_world.min(axis=0) + pts_world.max(axis=0)) / 2.0\n",
    "    new_world_to_eye = make_look_at_matrix(\n",
    "        orig_world_to_eye_xf, crop_center, camera_angle\n",
    "    )\n",
    "    if mirror_img_x:\n",
    "        mirrorx = np.eye(4, dtype=np.float32)\n",
    "        mirrorx[0, 0] = -1\n",
    "        new_world_to_eye = mirrorx @ new_world_to_eye\n",
    "\n",
    "\n",
    "    fx_fy, cx_cy = gen_intrinsics_from_bounding_pts(\n",
    "        transform3(new_world_to_eye, pts_world),\n",
    "        new_image_size[0],\n",
    "        new_image_size[1],\n",
    "    )\n",
    "    \n",
    "    fx_fy = focal_multiplier * fx_fy\n",
    "\n",
    "    return PinholePlaneCameraModel(\n",
    "        width=new_image_size[0],\n",
    "        height=new_image_size[1],\n",
    "        f=fx_fy,\n",
    "        c=cx_cy,\n",
    "        distort_coeffs=[],\n",
    "        camera_to_world_xf=np.linalg.inv(new_world_to_eye),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import from lib.tracker.perspective_crop.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "# import lib.common.camera as camera\n",
    "import numpy as np\n",
    "import torch\n",
    "#from lib.common.crop import gen_crop_parameters_from_points\n",
    "from lib.common.hand import HandModel, NUM_JOINTS_PER_HAND, RIGHT_HAND_INDEX\n",
    "from lib.common.hand_skinning import skin_landmarks\n",
    "\n",
    "#from .tracking_result import SingleHandPose\n",
    "\n",
    "def neutral_joint_angles(up: HandModel, lower_factor: float = 0.5) -> torch.Tensor:\n",
    "    joint_limits = up.joint_limits\n",
    "    assert joint_limits is not None\n",
    "    return joint_limits[..., 0] * lower_factor + joint_limits[..., 1] * (\n",
    "        1 - lower_factor\n",
    "    )\n",
    "\n",
    "\n",
    "def skin_landmarks_np(\n",
    "    hand_model: HandModel,\n",
    "    joint_angles: np.ndarray,\n",
    "    wrist_transforms: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    landmarks = skin_landmarks(\n",
    "        hand_model,\n",
    "        torch.from_numpy(joint_angles).float(),\n",
    "        torch.from_numpy(wrist_transforms).float(),\n",
    "    )\n",
    "    return landmarks.numpy()\n",
    "\n",
    "\n",
    "def landmarks_from_hand_pose(\n",
    "    hand_model: HandModel, hand_pose: SingleHandPose, hand_idx: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute 3D landmarks in the world space given the hand model and hand pose.\n",
    "    \"\"\"\n",
    "    xf = hand_pose.wrist_xform.copy()\n",
    "    # This function expects the user hand model to be a left hand.\n",
    "    if hand_idx == RIGHT_HAND_INDEX:\n",
    "        xf[:, 0] *= -1\n",
    "    landmarks = skin_landmarks_np(hand_model, hand_pose.joint_angles, xf)\n",
    "    return landmarks\n",
    "\n",
    "\n",
    "def rank_hand_visibility_in_cameras(\n",
    "    cameras: List[CameraModel],\n",
    "    hand_model: HandModel,\n",
    "    hand_pose: SingleHandPose,\n",
    "    hand_idx: int,\n",
    "    min_required_vis_landmarks: int,\n",
    ") -> List[int]:\n",
    "    landmarks_world = landmarks_from_hand_pose(\n",
    "        hand_model, hand_pose, hand_idx\n",
    "    )\n",
    "    \n",
    "    # list of number of 3D keypoints that project into views for each camera\n",
    "    n_landmarks_in_view = []\n",
    "    \n",
    "    # list of camera indices that can see enough hand points\n",
    "    ranked_cam_indices = []\n",
    "    for cam_idx, camera in enumerate(cameras):\n",
    "        # (21, 3)\n",
    "        landmarks_eye = camera.world_to_eye(landmarks_world)\n",
    "        \n",
    "        # (21, 2)\n",
    "        landmarks_win2 = camera.eye_to_window(landmarks_eye)\n",
    "        \n",
    "        n_visible = (\n",
    "            (landmarks_win2[..., 0] >= 0)\n",
    "            & (landmarks_win2[..., 0] <= camera.width - 1)\n",
    "            & (landmarks_win2[..., 1] >= 0)\n",
    "            & (landmarks_win2[..., 1] <= camera.height - 1)\n",
    "            & (landmarks_eye[..., 2] > 0)\n",
    "        ).sum()\n",
    "\n",
    "        n_landmarks_in_view.append(n_visible)\n",
    "        # Only push the cameras that can see enough hand points\n",
    "        if n_visible >= min_required_vis_landmarks:\n",
    "            ranked_cam_indices.append(cam_idx)\n",
    "\n",
    "    # print(\"ranked_cam_indices\", ranked_cam_indices)\n",
    "    # print(\"n_landmarks_in_view\", n_landmarks_in_view)   \n",
    "\n",
    "    #  Favor the view that sees more landmarks\n",
    "    ranked_cam_indices.sort(\n",
    "        reverse=True,\n",
    "        key=lambda x: n_landmarks_in_view[x],\n",
    "    )\n",
    "    # print(\"ranked_cam_indices\", ranked_cam_indices)\n",
    "    return ranked_cam_indices\n",
    "\n",
    "\n",
    "def _get_crop_points_from_hand_pose(\n",
    "    hand_model: HandModel,\n",
    "    gt_hand_pose: SingleHandPose,\n",
    "    hand_idx: int,\n",
    "    num_crop_points: int,\n",
    ") -> np.ndarray:\n",
    "    '''\n",
    "    args :\n",
    "        hand_model : HandModel\n",
    "        \n",
    "        gt_hand_pose : SingleHandPose\n",
    "        \n",
    "        hand_idx : int\n",
    "\n",
    "        num_crop_points : int\n",
    "            ㅅㅂ WTF does num_crop_points do?\n",
    "    \n",
    "    return :\n",
    "        np.ndarray of shape (num_crop_points, 3)\n",
    "        each row is a 3D keypoint in the 3D coordinate\n",
    "        (not sure world space or eye space)\n",
    "    '''\n",
    "    assert num_crop_points in [21, 42, 63]\n",
    "    neutral_hand_pose = SingleHandPose(\n",
    "        joint_angles=neutral_joint_angles(hand_model).numpy(),\n",
    "        wrist_xform=gt_hand_pose.wrist_xform,\n",
    "    )\n",
    "    open_hand_pose = SingleHandPose(\n",
    "        joint_angles=np.zeros(NUM_JOINTS_PER_HAND, dtype=np.float32),\n",
    "        wrist_xform=gt_hand_pose.wrist_xform,\n",
    "    )\n",
    "\n",
    "    crop_points = []\n",
    "    crop_points.append(landmarks_from_hand_pose(\n",
    "        hand_model, gt_hand_pose, hand_idx)\n",
    "    )\n",
    "    if num_crop_points > 21:\n",
    "        crop_points.append(\n",
    "            landmarks_from_hand_pose(\n",
    "                hand_model, neutral_hand_pose, hand_idx\n",
    "            )\n",
    "        )\n",
    "    if num_crop_points > 42:\n",
    "        crop_points.append(\n",
    "            landmarks_from_hand_pose(\n",
    "                hand_model, open_hand_pose, hand_idx\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    result = np.concatenate(crop_points, axis=0)\n",
    "    # print(\"crop_points\", result.shape)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def gen_crop_cameras_from_pose(\n",
    "    cameras: List[CameraModel],\n",
    "    camera_angles: List[float],\n",
    "    hand_model: HandModel,\n",
    "    hand_pose: SingleHandPose,\n",
    "    hand_idx: int,\n",
    "    num_crop_points: int,\n",
    "    new_image_size: Tuple[int, int],\n",
    "    max_view_num: Optional[int] = None,\n",
    "    sort_camera_index: bool = False,\n",
    "    focal_multiplier: float = 0.95,\n",
    "    mirror_right_hand: bool = True,\n",
    "    min_required_vis_landmarks: int = 19,\n",
    ") -> Dict[int, PinholePlaneCameraModel]:\n",
    "    \n",
    "    crop_cameras: Dict[int, PinholePlaneCameraModel] = {}\n",
    "\n",
    "    # keypoints coordinates in the world space\n",
    "    crop_points = _get_crop_points_from_hand_pose(\n",
    "        hand_model,\n",
    "        hand_pose,\n",
    "        hand_idx,\n",
    "        num_crop_points,\n",
    "    )\n",
    "    \n",
    "    # list of camera indices that can see enough hand points\n",
    "    # sorted by the number of landmarks that can be seen, descending\n",
    "    cam_indices = rank_hand_visibility_in_cameras(\n",
    "        cameras=cameras,\n",
    "        hand_model=hand_model,\n",
    "        hand_pose=hand_pose,\n",
    "        hand_idx=hand_idx,\n",
    "        min_required_vis_landmarks=min_required_vis_landmarks,\n",
    "    )\n",
    "\n",
    "    # cam_indices is sorted by the number of landmarks that can be seen, descending\n",
    "    # if sort_camera_index is True, then sort cam_indices\n",
    "    if sort_camera_index:\n",
    "        cam_indices = sorted(cam_indices)\n",
    "\n",
    "\n",
    "    for cam_idx in cam_indices:\n",
    "        crop_cameras[cam_idx] = gen_crop_parameters_from_points(\n",
    "            cameras[cam_idx],\n",
    "            crop_points,\n",
    "            new_image_size,\n",
    "            mirror_img_x=(mirror_right_hand and hand_idx == 1),\n",
    "            camera_angle=camera_angles[cam_idx],\n",
    "            focal_multiplier=focal_multiplier,\n",
    "        )\n",
    "        if len(crop_cameras) == max_view_num:\n",
    "            break\n",
    "\n",
    "    return crop_cameras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import from tracker.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracker.py\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import cv2\n",
    "\n",
    "# import lib.common.camera as camera\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "from lib.common.hand import HandModel, NUM_HANDS, scaled_hand_model\n",
    "\n",
    "from lib.data_utils import bundles\n",
    "\n",
    "from lib.models.regressor import RegressorOutput\n",
    "from lib.models.umetrack_model import InputFrameData, InputFrameDesc, InputSkeletonData\n",
    "\n",
    "\n",
    "MM_TO_M = 0.001\n",
    "M_TO_MM = 1000.0\n",
    "MIN_OBSERVED_LANDMARKS = 21\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "MAX_VIEW_NUM = 2\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ViewData:\n",
    "    image: np.ndarray\n",
    "    camera: CameraModel\n",
    "    camera_angle: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InputFrame:\n",
    "    views: List[ViewData]\n",
    "\n",
    "@dataclass\n",
    "class HandTrackerOpts:\n",
    "    \n",
    "    num_crop_points: int = 63\n",
    "    # can be [21, 42, 63]\n",
    "    # when performing perspective crop,\n",
    "    # 21: crop along detected pose ?\n",
    "    # 42: crop along detected pose + open pose\n",
    "    # 63: crop along detected pose + open pose + neutral pose\n",
    "    \n",
    "    enable_memory: bool = True\n",
    "    \n",
    "    use_stored_pose_for_crop: bool = True\n",
    "    \n",
    "    hand_ratio_in_crop: float = 0.95\n",
    "    \n",
    "    min_required_vis_landmarks: int = 19\n",
    "\n",
    "def _warp_image(\n",
    "    src_camera: CameraModel,\n",
    "    dst_camera: CameraModel,\n",
    "    src_image: np.ndarray,\n",
    "    interpolation: int = cv2.INTER_LINEAR,\n",
    "    depth_check: bool = True,\n",
    ") -> np.ndarray:\n",
    "    W, H = dst_camera.width, dst_camera.height\n",
    "    px, py = np.meshgrid(np.arange(W), np.arange(H))\n",
    "    dst_win_pts = np.column_stack((px.flatten(), py.flatten()))\n",
    "\n",
    "\n",
    "    # print(\"dst_camera.dostort\", type(dst_camera.distort))\n",
    "    # print(\"dst_camera.distort\", dst_camera.distort)\n",
    "    # print(isinstance(dst_camera.distort, NoDistortion))\n",
    "\n",
    "    dst_eye_pts = dst_camera.window_to_eye(dst_win_pts)\n",
    "    world_pts = dst_camera.eye_to_world(dst_eye_pts)\n",
    "    src_eye_pts = src_camera.world_to_eye(world_pts)\n",
    "    src_win_pts = src_camera.eye_to_window(src_eye_pts)\n",
    "\n",
    "    # Mask out points with negative z coordinates\n",
    "    if depth_check:\n",
    "        mask = src_eye_pts[:, 2] < 0\n",
    "        src_win_pts[mask] = -1\n",
    "\n",
    "    src_win_pts = src_win_pts.astype(np.float32)\n",
    "\n",
    "    map_x = src_win_pts[:, 0].reshape((H, W))\n",
    "    map_y = src_win_pts[:, 1].reshape((H, W))\n",
    "\n",
    "    warped_image = cv2.remap(src_image, map_x, map_y, interpolation)\n",
    "    \n",
    "    \n",
    "    return warped_image\n",
    "\n",
    "class HandTracker:\n",
    "    def __init__(self, model, opts: HandTrackerOpts) -> None:\n",
    "        self._device: str = \"cuda\" if torch.cuda.device_count() else \"cpu\"\n",
    "        \n",
    "        self._model = model\n",
    "        self._model.to(self._device)\n",
    "\n",
    "        self._input_size = np.array(self._model.getInputImageSizes())\n",
    "        self._num_crop_points = opts.num_crop_points\n",
    "        self._enable_memory = opts.enable_memory\n",
    "        self._hand_ratio_in_crop: float = opts.hand_ratio_in_crop\n",
    "        self._min_required_vis_landmarks: int = opts.min_required_vis_landmarks\n",
    "        self._valid_tracking_history = np.zeros(2, dtype=bool)\n",
    "\n",
    "    def reset_history(self) -> None:\n",
    "        self._valid_tracking_history[:] = False\n",
    "    def gen_crop_cameras_from_stereo_camera_with_window_hand_pose(\n",
    "        self,\n",
    "        camera_left: CameraModel,\n",
    "        camera_right: CameraModel,\n",
    "        window_hand_pose_left: Dict[int, np.ndarray],\n",
    "        window_hand_pose_right: Dict[int, np.ndarray],\n",
    "    ) :\n",
    "        \"\"\"\n",
    "        window coordinate : pixel coordinate\n",
    "        get crop_cameras from camera, view and 2D pose\n",
    "        \n",
    "        Assumstions :\n",
    "            camera is stereo camera. \n",
    "            2D Hand pose is already valid.\n",
    "        args :\n",
    "            camera_left  : camera.CameraModel\n",
    "            camera_right : camera.CameraModel\n",
    "            window_hand_pose_left  : Dict[int, np.ndarray]\n",
    "            window_hand_pose_right : Dict[int, np.ndarray]\n",
    "        \"\"\"\n",
    "        crop_camera_dict: Dict[int, Dict[int, PinholePlaneCameraModel]] = {}\n",
    "        \n",
    "        # perform for left camera\n",
    "        camera_left_world_to_eye = np.linalg.inv(camera_left.camera_to_world_xf)\n",
    "        for hand_idx, window_hand_pose in window_hand_pose_left.items():\n",
    "            # hand_idx      left : 0  right : 1\n",
    "            # window_hand_pose : np.array, (21, 2). window coordinate\n",
    "            crop_camera_dict_per_hand: Dict[int, PinholePlaneCameraModel] = {}\n",
    "            \n",
    "            world_hand_pose = camera_left.eye_to_world(\n",
    "                camera_left.window_to_eye(window_hand_pose[:, :2])\n",
    "            )   \n",
    "            \n",
    "            world_hand_pose_center = (\n",
    "                world_hand_pose.min(axis=0) + world_hand_pose.max(axis=0)\n",
    "            ) / 2\n",
    "            \n",
    "            new_world_to_eye = make_look_at_matrix(\n",
    "                camera_left_world_to_eye,\n",
    "                world_hand_pose_center,\n",
    "                0\n",
    "            )\n",
    "            if hand_idx == 1:\n",
    "                mirrorx = np.eye(4, dtype=np.float32)\n",
    "                mirrorx[0, 0] = -1\n",
    "                new_world_to_eye = mirrorx @ new_world_to_eye\n",
    "            \n",
    "            fx_fy, cx_cy = gen_intrinsics_from_bounding_pts(\n",
    "                transform3(new_world_to_eye, world_hand_pose),\n",
    "                self._input_size[0], self._input_size[1],\n",
    "            )\n",
    "            fx_fy = self._hand_ratio_in_crop * fx_fy\n",
    "            \n",
    "            new_cam = PinholePlaneCameraModel(\n",
    "                width=self._input_size[0],\n",
    "                height=self._input_size[1],\n",
    "                f=fx_fy,\n",
    "                c=cx_cy,\n",
    "                distort_coeffs = [],\n",
    "                camera_to_world_xf=np.linalg.inv(new_world_to_eye),\n",
    "            )\n",
    "            crop_camera_dict_per_hand[hand_idx] = new_cam\n",
    "            crop_camera_dict[hand_idx] = crop_camera_dict_per_hand\n",
    "            \n",
    "        camera_right_world_to_eye = np.linalg.inv(camera_right.camera_to_world_xf)\n",
    "        for hand_idx, window_hand_pose in window_hand_pose_right.items():\n",
    "            crop_camera_dict_per_hand = {}\n",
    "            \n",
    "            world_hand_pose = camera_right.eye_to_world(\n",
    "                camera_right.window_to_eye(window_hand_pose[:, :2])\n",
    "            )   \n",
    "            \n",
    "            world_hand_pose_center = (\n",
    "                world_hand_pose.min(axis=0) + world_hand_pose.max(axis=0)\n",
    "            ) / 2\n",
    "            \n",
    "            new_world_to_eye = make_look_at_matrix(\n",
    "                camera_right_world_to_eye,\n",
    "                world_hand_pose_center,\n",
    "                0\n",
    "            )\n",
    "            if hand_idx == 1:\n",
    "                mirrorx = np.eye(4, dtype=np.float32)\n",
    "                mirrorx[0, 0] = -1\n",
    "                new_world_to_eye = mirrorx @ new_world_to_eye\n",
    "            \n",
    "            fx_fy, cx_cy = gen_intrinsics_from_bounding_pts(\n",
    "                transform3(new_world_to_eye, world_hand_pose),\n",
    "                self._input_size[0], self._input_size[1],\n",
    "            )\n",
    "            fx_fy = self._hand_ratio_in_crop * fx_fy\n",
    "            \n",
    "            new_cam = PinholePlaneCameraModel(\n",
    "                width=self._input_size[0],\n",
    "                height=self._input_size[1],\n",
    "                f=fx_fy,\n",
    "                c=cx_cy,\n",
    "                distort_coeffs = [],\n",
    "                camera_to_world_xf=np.linalg.inv(new_world_to_eye),\n",
    "            )\n",
    "            crop_camera_dict_per_hand[hand_idx] = new_cam\n",
    "            if hand_idx in crop_camera_dict :\n",
    "                crop_camera_dict[hand_idx].update(crop_camera_dict_per_hand)\n",
    "            else :\n",
    "                crop_camera_dict[hand_idx] = crop_camera_dict_per_hand\n",
    "            \n",
    "        return crop_camera_dict\n",
    "            \n",
    "            # fig = go.Figure()\n",
    "            # fig.add_trace(go.Scatter3d(\n",
    "            #     x=world_hand_pose[:, 0], y=world_hand_pose[:, 1], z=world_hand_pose[:, 2],\n",
    "            #     mode='markers',\n",
    "            #     marker=dict(size=5, color='blue'),\n",
    "            #     name='World Hand Pose'\n",
    "            # ))\n",
    "            # camera_left_pos = camera_left.camera_to_world_xf[:3, 3]\n",
    "            # fig.add_trace(go.Scatter3d(\n",
    "            #     x=[camera_left_pos[0]], y=[camera_left_pos[1]], z=[camera_left_pos[2]],\n",
    "            #     mode='markers',\n",
    "            #     marker=dict(size=10, color='red', symbol='square'),\n",
    "            #     name='Camera Left'\n",
    "            # ))\n",
    "            # camera_right_pos = camera_right.camera_to_world_xf[:3, 3]\n",
    "            # fig.add_trace(go.Scatter3d(\n",
    "            #     x=[camera_right_pos[0]], y=[camera_right_pos[1]], z=[camera_right_pos[2]],\n",
    "            #     mode='markers',\n",
    "            #     marker=dict(size=10, color='green', symbol='square'),\n",
    "            #     name='Camera Right'\n",
    "            # ))\n",
    "            # fig.update_layout(\n",
    "            #     scene=dict(\n",
    "            #         xaxis_title='X',\n",
    "            #         yaxis_title='Y',\n",
    "            #         zaxis_title='Z',\n",
    "            #         aspectmode='data'\n",
    "            #     ),\n",
    "            #     title='3D Visualization of World Hand Pose and Cameras'\n",
    "            # )\n",
    "            # fig.show()\n",
    "            \n",
    "            \n",
    "        \n",
    "    def gen_crop_cameras_analysis(\n",
    "        self,\n",
    "        #cameras: List[camera.CameraModel],\n",
    "        input_frame,\n",
    "        camera_angles: List[float],\n",
    "        hand_model: HandModel,\n",
    "        gt_tracking: Dict[int, SingleHandPose],\n",
    "        min_num_crops: int,\n",
    "    ) -> Dict[int, Dict[int, PinholePlaneCameraModel]]:\n",
    "        \"\"\"\n",
    "        functions to analyze the crop cameras\n",
    "        \"\"\"\n",
    "    \n",
    "        cameras = [view.camera for view in input_frame.views]\n",
    "        \n",
    "        crop_cameras: Dict[int, Dict[int, PinholePlaneCameraModel]] = {}\n",
    "        if not gt_tracking:\n",
    "            return crop_cameras\n",
    "\n",
    "        # evaluate for each detected hands.\n",
    "        for hand_idx, gt_hand_pose in gt_tracking.items():\n",
    "            if gt_hand_pose.hand_confidence < CONFIDENCE_THRESHOLD:\n",
    "                continue\n",
    "\n",
    "            # contains croped cameras in multi origin views for single hand\n",
    "            crop_camera_dict_per_hand: Dict[int, PinholePlaneCameraModel] = {}\n",
    "            \n",
    "            # contains camera indexes that target hand is visible in       \n",
    "            valid_cam_index_list = sorted(rank_hand_visibility_in_cameras(\n",
    "                cameras,\n",
    "                hand_model,\n",
    "                gt_hand_pose,\n",
    "                hand_idx,\n",
    "                self._min_required_vis_landmarks,\n",
    "            ))\n",
    "\n",
    "            world_hand_pose = _get_crop_points_from_hand_pose(\n",
    "                hand_model,\n",
    "                gt_hand_pose,\n",
    "                hand_idx,\n",
    "                self._num_crop_points,\n",
    "                #21\n",
    "            )\n",
    "            \n",
    "            for valid_cam_idx in valid_cam_index_list:\n",
    "                orig_cam = cameras[valid_cam_idx]\n",
    "                orig_cam_angle = camera_angles[valid_cam_idx]\n",
    "                \n",
    "                orig_world_to_eye_xf = np.linalg.inv(orig_cam.camera_to_world_xf)\n",
    "                \n",
    "                world_hand_pose_center = (\n",
    "                    world_hand_pose.min(axis=0) + world_hand_pose.max(axis=0)\n",
    "                ) / 2\n",
    "                \n",
    "                window_hand_pose = orig_cam.eye_to_window(\n",
    "                    orig_cam.world_to_eye(world_hand_pose)\n",
    "                ).astype(int)\n",
    "                \n",
    "                \n",
    "                # plt.imshow(input_frame.views[valid_cam_idx].image, cmap=\"gray\")\n",
    "                # plt.scatter(window_hand_pose[:, 0], window_hand_pose[:, 1])\n",
    "                # plt.show()\n",
    "                \n",
    "                new_world_to_eye = make_look_at_matrix(\n",
    "                    orig_world_to_eye_xf,\n",
    "                    world_hand_pose_center,\n",
    "                    orig_cam_angle\n",
    "                )\n",
    "                \n",
    "                if hand_idx == 1:\n",
    "                    mirrorx = np.eye(4, dtype=np.float32)\n",
    "                    mirrorx[0, 0] = -1\n",
    "                    new_world_to_eye = mirrorx @ new_world_to_eye\n",
    "                \n",
    "                fx_fy, cx_cy = gen_intrinsics_from_bounding_pts(\n",
    "                    transform3(new_world_to_eye, world_hand_pose),\n",
    "                    self._input_size[0], self._input_size[1],\n",
    "                )\n",
    "                \n",
    "                fx_fy = self._hand_ratio_in_crop * fx_fy\n",
    "                \n",
    "                new_cam = PinholePlaneCameraModel(\n",
    "                    width=self._input_size[0],\n",
    "                    height=self._input_size[1],\n",
    "                    f=fx_fy,\n",
    "                    c=cx_cy,\n",
    "                    distort_coeffs = [],\n",
    "                    camera_to_world_xf=np.linalg.inv(new_world_to_eye),\n",
    "                )\n",
    "                crop_camera_dict_per_hand[valid_cam_idx] = new_cam\n",
    "\n",
    "            crop_cameras[hand_idx] = crop_camera_dict_per_hand\n",
    "\n",
    "        # Remove empty crop_cameras\n",
    "        del_list = []\n",
    "        for hand_idx, per_hand_crop_cameras in crop_cameras.items():\n",
    "            if not per_hand_crop_cameras or len(per_hand_crop_cameras) < min_num_crops:\n",
    "                del_list.append(hand_idx)\n",
    "        for hand_idx in del_list:\n",
    "            del crop_cameras[hand_idx]\n",
    "\n",
    "        return crop_cameras\n",
    "\n",
    "\n",
    "    def gen_crop_cameras(\n",
    "        self,\n",
    "        cameras: List[CameraModel],\n",
    "        camera_angles: List[float],\n",
    "        hand_model: HandModel,\n",
    "        gt_tracking: Dict[int, SingleHandPose],\n",
    "        min_num_crops: int,\n",
    "    ) -> Dict[int, Dict[int, PinholePlaneCameraModel]]:\n",
    "    \n",
    "        '''\n",
    "        args :\n",
    "        return :\n",
    "            crop_cameras: Dict[int, Dict[int, camera.PinholePlaneCameraModel]]\n",
    "                key : hand_idx\n",
    "                value : Dict[int, camera.PinholePlaneCameraModel]\n",
    "                    key : cam_idx\n",
    "                    value : camera.PinholePlaneCameraModel\n",
    "        '''\n",
    "        crop_cameras: Dict[int, Dict[int, PinholePlaneCameraModel]] = {}\n",
    "        if not gt_tracking:\n",
    "            return crop_cameras\n",
    "\n",
    "        for hand_idx, gt_hand_pose in gt_tracking.items():\n",
    "            if gt_hand_pose.hand_confidence < CONFIDENCE_THRESHOLD:\n",
    "                continue\n",
    "            crop_cameras[hand_idx] = gen_crop_cameras_from_pose(\n",
    "                cameras,\n",
    "                camera_angles,\n",
    "                hand_model,\n",
    "                gt_hand_pose,\n",
    "                hand_idx,\n",
    "                self._num_crop_points,\n",
    "                self._input_size,\n",
    "                max_view_num=MAX_VIEW_NUM,\n",
    "                sort_camera_index=True,\n",
    "                focal_multiplier=self._hand_ratio_in_crop,\n",
    "                mirror_right_hand=True,\n",
    "                min_required_vis_landmarks=self._min_required_vis_landmarks,\n",
    "            )\n",
    "\n",
    "        # Remove empty crop_cameras\n",
    "        del_list = []\n",
    "        for hand_idx, per_hand_crop_cameras in crop_cameras.items():\n",
    "            if not per_hand_crop_cameras or len(per_hand_crop_cameras) < min_num_crops:\n",
    "                del_list.append(hand_idx)\n",
    "        for hand_idx in del_list:\n",
    "            del crop_cameras[hand_idx]\n",
    "\n",
    "        return crop_cameras\n",
    "\n",
    "    def track_frame(\n",
    "        self,\n",
    "        sample: InputFrame,\n",
    "        hand_model: HandModel,\n",
    "        crop_cameras: Dict[int, Dict[int, PinholePlaneCameraModel]],\n",
    "    ) -> TrackingResult:\n",
    "        if not crop_cameras:\n",
    "            # Frame without hands\n",
    "            self.reset_history()\n",
    "            return TrackingResult()\n",
    "\n",
    "        frame_data, frame_desc, skeleton_data = self._make_inputs(\n",
    "            sample, hand_model, crop_cameras\n",
    "        )\n",
    "\n",
    "        print(skeleton_data)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            regressor_output = bundles.to_device(\n",
    "                self._model.regress_pose_use_skeleton(\n",
    "                    frame_data, frame_desc, skeleton_data\n",
    "                ),\n",
    "                torch.device(\"cpu\"),\n",
    "            )\n",
    "\n",
    "        tracking_result = self._gen_tracking_result(\n",
    "            regressor_output,\n",
    "            frame_desc.hand_idx.cpu().numpy(),\n",
    "            crop_cameras,\n",
    "        )\n",
    "        return tracking_result\n",
    "\n",
    "    def track_frame_analysis(\n",
    "        self,\n",
    "        sample: InputFrame,\n",
    "        hand_model: HandModel,\n",
    "        crop_cameras: Dict[int, Dict[int, PinholePlaneCameraModel]],\n",
    "        gt_tracking: Dict[int, SingleHandPose],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        args :\n",
    "            sample : InputFrame\n",
    "            hand_model : HandModel\n",
    "                translation 이 밀리미터임\n",
    "            crop_cameras : Dict[int, Dict[int, PinholePlaneCameraModel]]\n",
    "        \"\"\"\n",
    "        if not crop_cameras:\n",
    "            # Frame without hands\n",
    "            self.reset_history()\n",
    "            return TrackingResult()\n",
    "        \n",
    "        # < ================================================\n",
    "        # < HandTracker._make_inputs\n",
    "        \n",
    "        image_idx = 0\n",
    "        left_images = []\n",
    "        intrinsics = []\n",
    "        extrinsics_xf = []\n",
    "        \n",
    "        sample_range_n_hands = []\n",
    "        hand_indices = []\n",
    "        \n",
    "        for hand_idx, crop_camera_info in crop_cameras.items():\n",
    "            sample_range_start = image_idx\n",
    "            for cam_idx, crop_camera in crop_camera_info.items():\n",
    "                view_data = sample.views[cam_idx]\n",
    "                \n",
    "                # << ================================================\n",
    "                # crop_image = _warp_image(\n",
    "                #     view_data.camera,\n",
    "                #     crop_camera,\n",
    "                #     view_data.image\n",
    "                # )\n",
    "                # << _warp_image\n",
    "                \n",
    "                depth_check = True\n",
    "                \n",
    "                W, H = crop_camera.width, crop_camera.height\n",
    "                px, py = np.meshgrid(np.arange(W), np.arange(H))\n",
    "                dst_win_points = np.column_stack([px.flatten(), py.flatten()])\n",
    "                \n",
    "                dst_eye_pts = crop_camera.window_to_eye(dst_win_points)\n",
    "                world_pts = crop_camera.eye_to_world(dst_eye_pts)\n",
    "                src_eye_pts = view_data.camera.world_to_eye(world_pts)\n",
    "                src_win_pts = view_data.camera.eye_to_window(src_eye_pts)\n",
    "                \n",
    "                if depth_check :\n",
    "                    mask = src_eye_pts[:, 2] < 0\n",
    "                    src_win_pts[mask] = -1\n",
    "                    \n",
    "                src_win_pts = src_win_pts.astype(np.float32)\n",
    "                map_x = src_win_pts[:, 0].reshape((H, W))\n",
    "                map_y = src_win_pts[:, 1].reshape((H, W))\n",
    "                \n",
    "                crop_image = cv2.remap(\n",
    "                    view_data.image, map_x, map_y, cv2.INTER_LINEAR\n",
    "                )\n",
    "                \n",
    "                # >> _warp_image\n",
    "                # >> ================================================\n",
    "                left_images.append(crop_image.astype(np.float32) / 255.0)\n",
    "                intrinsics.append(crop_camera.uv_to_window_matrix())\n",
    "                \n",
    "                crop_world_to_eye_xf = np.linalg.inv(\n",
    "                    crop_camera.camera_to_world_xf\n",
    "                )\n",
    "                crop_world_to_eye_xf[:3, 3] *= MM_TO_M\n",
    "                extrinsics_xf.append(crop_world_to_eye_xf)\n",
    "                \n",
    "                image_idx += 1\n",
    "                \n",
    "            if image_idx > sample_range_start:\n",
    "                hand_indices.append(hand_idx)\n",
    "                \n",
    "                sample_range_n_hands.append(np.array(\n",
    "                    [sample_range_start, image_idx]\n",
    "                ))\n",
    "                \n",
    "        # visualize\n",
    "        # project gt_tracking to crop_cameras\n",
    "        color_list = ['r', 'g', 'b', 'y', 'm', 'c'] \n",
    "        for hand_idx, (start_idx, end_idx) in zip(hand_indices, sample_range_n_hands):\n",
    "            \n",
    "            plt.figure(figsize=(20, 10))\n",
    "            for i in range(start_idx, end_idx):\n",
    "                plt.subplot(1, end_idx - start_idx, i - start_idx + 1)\n",
    "                plt.imshow(left_images[i], cmap='gray')\n",
    "                \n",
    "                world_to_eye_xf = extrinsics_xf[i]  \n",
    "                # meter to mm\n",
    "                world_to_eye_xf[:3, 3] *= M_TO_MM\n",
    "                \n",
    "                eye_to_window_xf = intrinsics[i]\n",
    "                \n",
    "                keypoints_world = landmarks_from_hand_pose(\n",
    "                    hand_model, gt_tracking[hand_idx], hand_idx\n",
    "                )\n",
    "                keypoints_world_homogeneous = np.hstack(\n",
    "                    [keypoints_world, np.ones((len(keypoints_world), 1))]\n",
    "                )\n",
    "                \n",
    "                keypoints_eye_homogeneous = world_to_eye_xf @ keypoints_world_homogeneous.T\n",
    "                keypoints_eye = keypoints_eye_homogeneous[:3, :].T\n",
    "                \n",
    "                keypoints_window_homogeneous = eye_to_window_xf @ keypoints_eye.T\n",
    "                keypoints_window = keypoints_window_homogeneous[:2, :].T / keypoints_window_homogeneous[2:3, :].T\n",
    "                \n",
    "                plt.scatter(\n",
    "                    keypoints_window[:, 0],\n",
    "                    keypoints_window[:, 1],\n",
    "                    c=color_list[hand_idx],\n",
    "                    s=50,\n",
    "                    edgecolors='k',\n",
    "                    linewidths=3,\n",
    "                )\n",
    "            plt.show()\n",
    "        \n",
    "        \n",
    "        hand_indices = np.array(hand_indices)\n",
    "        frame_data = InputFrameData(\n",
    "            left_images=torch.from_numpy(np.stack(left_images)).float(),\n",
    "            intrinsics=torch.from_numpy(np.stack(intrinsics)).float(),\n",
    "            extrinsics_xf=torch.from_numpy(np.stack(extrinsics_xf)).float(),\n",
    "        )\n",
    "        frame_desc = InputFrameDesc(\n",
    "            sample_range=torch.from_numpy(np.stack(\n",
    "                sample_range_n_hands\n",
    "            )).long(),\n",
    "            memory_idx=torch.from_numpy(hand_indices).long(),\n",
    "            # use memory if the hand is previously valid\n",
    "            use_memory=torch.from_numpy(\n",
    "                self._valid_tracking_history[hand_indices]\n",
    "            ).bool(),\n",
    "            hand_idx=torch.from_numpy(hand_indices).long(),\n",
    "        )\n",
    "        skeleton_data = None\n",
    "        if hand_model is not None:\n",
    "            # m -> mm\n",
    "            hand_model_m = scaled_hand_model(hand_model, MM_TO_M)\n",
    "            skeleton_data = InputSkeletonData(\n",
    "                joint_rotation_axes=hand_model_m.joint_rotation_axes.float(),\n",
    "                joint_rest_positions=hand_model_m.joint_rest_positions.float(),\n",
    "            )\n",
    "        frame_data, frame_desc, skeleton_data = bundles.to_device(\n",
    "            (\n",
    "            frame_data, frame_desc, skeleton_data\n",
    "            ), self._device\n",
    "        )\n",
    "\n",
    "\n",
    "        # > _make_inputs\n",
    "        # > ================================================\n",
    "        \n",
    "        print(skeleton_data)\n",
    "\n",
    "\n",
    "        with torch.no_grad():            \n",
    "            regressor_output = bundles.to_device(\n",
    "                self._model.regress_pose_use_skeleton(\n",
    "                    frame_data, frame_desc, skeleton_data\n",
    "                ),\n",
    "                torch.device(\"cpu\"),\n",
    "            )\n",
    "\n",
    "        #print(regressor_output)\n",
    "\n",
    "        # tracking_result = self._gen_tracking_result(\n",
    "        #     regressor_output,\n",
    "        #     frame_desc.hand_idx.cpu().numpy(),  -> hand_indices\n",
    "        #     crop_cameras,\n",
    "        # )\n",
    "        # return tracking_result\n",
    "\n",
    "        # < ================================================\n",
    "        # < _gen_tracking_result\n",
    "        hand_indices_np = frame_desc.hand_idx.cpu().numpy()\n",
    "\n",
    "        output_joint_angles = regressor_output.joint_angles.to(\"cpu\").numpy()\n",
    "        \n",
    "        # print(output_joint_angles)\n",
    "\n",
    "        output_wrist_xforms = regressor_output.wrist_xfs.to(\"cpu\").numpy()\n",
    "        output_wrist_xforms[..., :3, 3] *= M_TO_MM\n",
    "        output_scales = None\n",
    "        if regressor_output.skel_scales is not None:\n",
    "            output_scales = regressor_output.skel_scales.to(\"cpu\").numpy()\n",
    "\n",
    "        hand_poses = {}\n",
    "        num_views = {}\n",
    "        predicted_scales = {}\n",
    "        \n",
    "        for output_idx, hand_idx in enumerate(hand_indices_np):\n",
    "            raw_handpose = SingleHandPose(\n",
    "                joint_angles=output_joint_angles[output_idx],\n",
    "                wrist_xform=output_wrist_xforms[output_idx],\n",
    "                hand_confidence=1.0,\n",
    "            )\n",
    "            hand_poses[hand_idx] = raw_handpose\n",
    "            num_views[hand_idx] = len(crop_cameras[hand_idx])\n",
    "            if output_scales is not None:\n",
    "                predicted_scales[hand_idx] = output_scales[output_idx]\n",
    "\n",
    "        for hand_idx in range(NUM_HANDS):\n",
    "            hand_valid = False\n",
    "            if hand_idx in hand_poses:\n",
    "                self._valid_tracking_history[hand_idx] = True\n",
    "                hand_valid = True\n",
    "            if hand_valid:\n",
    "                continue\n",
    "            self._valid_tracking_history[hand_idx] = False\n",
    "\n",
    "        return TrackingResult(\n",
    "            hand_poses=hand_poses,\n",
    "            num_views=num_views,\n",
    "            predicted_scales=predicted_scales,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def track_frame_and_calibrate_scale(\n",
    "        self,\n",
    "        sample: InputFrame,\n",
    "        crop_cameras: Dict[int, Dict[int, PinholePlaneCameraModel]],\n",
    "    ) -> TrackingResult:\n",
    "        if not crop_cameras:\n",
    "            # Frame without hands\n",
    "            self.reset_history()\n",
    "            return TrackingResult()\n",
    "        frame_data, frame_desc, _ = self._make_inputs(sample, None, crop_cameras)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            regressor_output = bundles.to_device(\n",
    "                self._model.regress_pose_pred_skel_scale(frame_data, frame_desc),\n",
    "                torch.device(\"cpu\"),\n",
    "            )\n",
    "\n",
    "        tracking_result = self._gen_tracking_result(\n",
    "            regressor_output,\n",
    "            frame_desc.hand_idx.cpu().numpy(),\n",
    "            crop_cameras,\n",
    "        )\n",
    "        return tracking_result\n",
    "\n",
    "    def _make_inputs(\n",
    "        self,\n",
    "        sample: InputFrame,\n",
    "        hand_model_mm: Optional[HandModel],\n",
    "        crop_cameras: Dict[int, Dict[int, PinholePlaneCameraModel]],\n",
    "    ):\n",
    "        '''\n",
    "        args:\n",
    "            sample: InputFrame\n",
    "            hand_model_mm: HandModel\n",
    "                mm은 밀리미터인건가?\n",
    "            crop_cameras: Dict[int, Dict[int, PinholePlaneCameraModel]]\n",
    "        return:\n",
    "            frame_data: InputFrameData\n",
    "            frame_desc: InputFrameDesc\n",
    "            skeleton_data: InputSkeletonData\n",
    "        '''\n",
    "        image_idx = 0\n",
    "        left_images = []\n",
    "        intrinsics = []\n",
    "        extrinsics_xf = []\n",
    "        sample_range_n_hands = []\n",
    "        hand_indices = []\n",
    "        \n",
    "        for hand_idx, crop_camera_info in crop_cameras.items():\n",
    "            \n",
    "            sample_range_start = image_idx\n",
    "            for cam_idx, crop_camera in crop_camera_info.items():\n",
    "                view_data = sample.views[cam_idx]\n",
    "                crop_image = _warp_image(view_data.camera, crop_camera, view_data.image)\n",
    "                left_images.append(crop_image.astype(np.float32) / 255.0)\n",
    "                intrinsics.append(crop_camera.uv_to_window_matrix())\n",
    "\n",
    "                crop_world_to_eye_xf = np.linalg.inv(crop_camera.camera_to_world_xf)\n",
    "                crop_world_to_eye_xf[:3, 3] *= MM_TO_M\n",
    "                extrinsics_xf.append(crop_world_to_eye_xf)\n",
    "\n",
    "                image_idx += 1\n",
    "\n",
    "            if image_idx > sample_range_start:\n",
    "                hand_indices.append(hand_idx)\n",
    "                \n",
    "                # sample_range_start 부터 image_idx 까지가 특정 손에 대한 정보\n",
    "                # 이걸 가지고 left_images, intrinsics 등을 인덱싱하는 듯.\n",
    "                sample_range_n_hands.append(np.array(\n",
    "                    [sample_range_start, image_idx]\n",
    "                ))\n",
    "\n",
    "        hand_indices = np.array(hand_indices)\n",
    "        frame_data = InputFrameData(\n",
    "            left_images=torch.from_numpy(np.stack(left_images)).float(),\n",
    "            intrinsics=torch.from_numpy(np.stack(intrinsics)).float(),\n",
    "            extrinsics_xf=torch.from_numpy(np.stack(extrinsics_xf)).float(),\n",
    "        )\n",
    "        frame_desc = InputFrameDesc(\n",
    "            sample_range=torch.from_numpy(np.stack(sample_range_n_hands)).long(),\n",
    "            memory_idx=torch.from_numpy(hand_indices).long(),\n",
    "            # use memory if the hand is previously valid\n",
    "            use_memory=torch.from_numpy(\n",
    "                self._valid_tracking_history[hand_indices]\n",
    "            ).bool(),\n",
    "            hand_idx=torch.from_numpy(hand_indices).long(),\n",
    "        )\n",
    "        skeleton_data = None\n",
    "        if hand_model_mm is not None:\n",
    "            # m -> mm\n",
    "            hand_model_m = scaled_hand_model(hand_model_mm, MM_TO_M)\n",
    "            skeleton_data = InputSkeletonData(\n",
    "                joint_rotation_axes=hand_model_m.joint_rotation_axes.float(),\n",
    "                joint_rest_positions=hand_model_m.joint_rest_positions.float(),\n",
    "            )\n",
    "        return bundles.to_device((frame_data, frame_desc, skeleton_data), self._device)\n",
    "\n",
    "    def _gen_tracking_result(\n",
    "        self,\n",
    "        regressor_output: RegressorOutput,\n",
    "        hand_indices: np.ndarray,\n",
    "        crop_cameras: Dict[int, Dict[int, PinholePlaneCameraModel]],\n",
    "    ) -> TrackingResult:\n",
    "\n",
    "        output_joint_angles = regressor_output.joint_angles.to(\"cpu\").numpy()\n",
    "        \n",
    "        # print(output_joint_angles)\n",
    "\n",
    "\n",
    "        output_wrist_xforms = regressor_output.wrist_xfs.to(\"cpu\").numpy()\n",
    "        output_wrist_xforms[..., :3, 3] *= M_TO_MM\n",
    "        output_scales = None\n",
    "        if regressor_output.skel_scales is not None:\n",
    "            output_scales = regressor_output.skel_scales.to(\"cpu\").numpy()\n",
    "\n",
    "        hand_poses = {}\n",
    "        num_views = {}\n",
    "        predicted_scales = {}\n",
    "\n",
    "        for output_idx, hand_idx in enumerate(hand_indices):\n",
    "            raw_handpose = SingleHandPose(\n",
    "                joint_angles=output_joint_angles[output_idx],\n",
    "                wrist_xform=output_wrist_xforms[output_idx],\n",
    "                hand_confidence=1.0,\n",
    "            )\n",
    "            hand_poses[hand_idx] = raw_handpose\n",
    "            num_views[hand_idx] = len(crop_cameras[hand_idx])\n",
    "            if output_scales is not None:\n",
    "                predicted_scales[hand_idx] = output_scales[output_idx]\n",
    "\n",
    "        for hand_idx in range(NUM_HANDS):\n",
    "            hand_valid = False\n",
    "            if hand_idx in hand_poses:\n",
    "                self._valid_tracking_history[hand_idx] = True\n",
    "                hand_valid = True\n",
    "            if hand_valid:\n",
    "                continue\n",
    "            self._valid_tracking_history[hand_idx] = False\n",
    "\n",
    "        return TrackingResult(\n",
    "            hand_poses=hand_poses,\n",
    "            num_views=num_views,\n",
    "            predicted_scales=predicted_scales,\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def visualize_crop_cameras(\n",
    "        self,\n",
    "        input_frame,\n",
    "        window_hand_pose_left,\n",
    "        window_hand_pose_right,\n",
    "        crop_camera_dict: Dict[int, Dict[int, PinholePlaneCameraModel]],\n",
    "    ):\n",
    "        \n",
    "        color_list = ['red', 'green'] \n",
    "        \n",
    "        window_hand_pose_list = [window_hand_pose_left, window_hand_pose_right]\n",
    "    \n",
    "        for cam_idx, view_data in enumerate(input_frame.views) :\n",
    "            print(cam_idx)\n",
    "            orig_image = view_data.image\n",
    "            orig_camera = view_data.camera\n",
    "            window_hand_pose = window_hand_pose_list[cam_idx]\n",
    "            \n",
    "            # process fror left camera\n",
    "            plt.figure(figsize=(20, 20))\n",
    "            for idx, (hand_idx, keypoints_window) in enumerate(window_hand_pose.items()):\n",
    "                keypoints_orig_eye = orig_camera.window_to_eye(keypoints_window[:, :2])\n",
    "                keypoints_orig_world = orig_camera.eye_to_world(keypoints_orig_eye)\n",
    "                    \n",
    "                keypoints_orig_eye_reprojected = orig_camera.world_to_eye(\n",
    "                    keypoints_orig_world\n",
    "                )\n",
    "                keypoints_orig_window_reprojected = orig_camera.eye_to_window(\n",
    "                    keypoints_orig_eye_reprojected\n",
    "                )\n",
    "        \n",
    "                # visualize original and reprojected keypoints\n",
    "                plt.subplot(1, len(window_hand_pose), idx+1)\n",
    "                plt.title(f\"Hand {hand_idx}, green: original, red: reprojected\")\n",
    "                plt.imshow(orig_image, cmap='gray')\n",
    "                \n",
    "                plt.scatter(\n",
    "                    keypoints_window[:, 0],\n",
    "                    keypoints_window[:, 1],\n",
    "                    c=\"green\",\n",
    "                    s=20,\n",
    "                    edgecolors=\"k\",\n",
    "                    linewidths=1,\n",
    "                )\n",
    "                \n",
    "                plt.scatter(\n",
    "                    keypoints_orig_window_reprojected[:, 0],\n",
    "                    keypoints_orig_window_reprojected[:, 1],\n",
    "                    c=\"red\",    \n",
    "                    s=20,\n",
    "                    edgecolors=\"k\",\n",
    "                    linewidths=1,\n",
    "                )\n",
    "   \n",
    "                    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "logging.basicConfig(level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# model_name = \"pretrained_weights.torch\"\n",
    "# model_path = os.path.join(\".\", \"pretrained_models\", model_name)\n",
    "# model = load_pretrained_model(model_path)\n",
    "# model.eval()\n",
    "# tracker = HandTracker(model, HandTrackerOpts())\n",
    "\n",
    "DATA_PATH = \"sample_data/recording_00.mp4\"\n",
    "image_pose_stream = SyncedImagePoseStream(DATA_PATH)\n",
    "hand_model = image_pose_stream._hand_pose_labels.hand_model\n",
    "\n",
    "image_pose_list = []\n",
    "for (input_frame, gt_tracking) in image_pose_stream:\n",
    "    image_pose_list.append((input_frame, gt_tracking))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_model.landmark_rest_positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse and analyze GT annotatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 20\n",
    "\n",
    "input_frame, gt_tracking = image_pose_list[idx]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for i, view in enumerate(input_frame.views):\n",
    "    plt.subplot(1, len(input_frame.views), i+1)\n",
    "    plt.imshow(view.image, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(len(gt_tracking), gt_tracking[0].wrist_xform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoint_pos_3D = landmarks_from_hand_pose(\n",
    "    image_pose_stream._hand_pose_labels.hand_model,\n",
    "    gt_tracking[0],\n",
    "    0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_frame.views[0].camera.distortion_model._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_frame.views[0].camera.distortion_model.k1)\n",
    "print(input_frame.views[0].camera.distortion_model.k2)\n",
    "print(input_frame.views[0].camera.distortion_model.k3)\n",
    "print(input_frame.views[0].camera.distortion_model.k4)\n",
    "print(input_frame.views[0].camera.distortion_model.p1)\n",
    "print(input_frame.views[0].camera.distortion_model.p2)\n",
    "print(input_frame.views[0].camera.distortion_model.k5)\n",
    "print(input_frame.views[0].camera.distortion_model.k6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Input (image, annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = \"\"\"\n",
    "def create_camera_visualization(ax, camera_to_world_xf):\n",
    "    camera_pos = camera_to_world_xf[:3, 3]\n",
    "    camera_x = camera_to_world_xf[:3, 0]\n",
    "    camera_y = camera_to_world_xf[:3, 1]\n",
    "    camera_z = camera_to_world_xf[:3, 2]\n",
    "    \n",
    "    ax.scatter(*camera_pos, color='red', s=50, label='Camera')\n",
    "    \n",
    "    scale = 10\n",
    "    for axis, color in zip([camera_x, camera_y, camera_z], ['red', 'green', 'blue']):\n",
    "        ax.quiver(*camera_pos, *axis, length=scale, color=color, linewidth=2)\n",
    "\n",
    "def create_keypoint_visualization(ax, keypoint_pos_3D):\n",
    "    print(keypoint_pos_3D.shape)\n",
    "    # keypoint_pos_3D is 21 x 3\n",
    "    # ax.scatter(\n",
    "    #     keypoint_pos_3D[:, 0],\n",
    "    #     keypoint_pos_3D[:, 1],\n",
    "    #     keypoint_pos_3D[:, 2],\n",
    "    #     color='blue',\n",
    "    #     label='Keypoint'\n",
    "    # )\n",
    "    \n",
    "    color_list = [\n",
    "        'blue', 'green', 'red', 'cyan', 'magenta',\n",
    "        'yellow', 'black', 'white', 'purple', 'orange',\n",
    "        'brown', 'pink', 'gray', 'olive', 'cyan',\n",
    "        'magenta', 'yellow', 'black', 'white', 'purple',\n",
    "        'orange', # 'brown', 'pink', 'gray', 'olive'\n",
    "    ]\n",
    "    # plot each keypoints in different color\n",
    "    ax.scatter(\n",
    "        keypoint_pos_3D[:, 0],\n",
    "        keypoint_pos_3D[:, 1],\n",
    "        keypoint_pos_3D[:, 2],\n",
    "        color=color_list,\n",
    "        label='Keypoint'\n",
    "    )\n",
    "    \n",
    "\n",
    "def update_plot(frame_idx):    \n",
    "    input_frame, gt_tracking = image_pose_list[frame_idx]\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    for view in input_frame.views:\n",
    "        create_camera_visualization(ax, view.camera.camera_to_world_xf)\n",
    "\n",
    "    for hand_idx, gt_hand_pose in gt_tracking.items():\n",
    "        keypoint_pos_3D = landmarks_from_hand_pose(\n",
    "            image_pose_stream._hand_pose_labels.hand_model,\n",
    "            gt_hand_pose,\n",
    "            hand_idx\n",
    "        )\n",
    "        create_keypoint_visualization(ax, keypoint_pos_3D)\n",
    "        break\n",
    "\n",
    "\n",
    "    # for hand_idx, gt_hand_pose in gt_tracking.items():\n",
    "    #     wrist_xform = gt_hand_pose.wrist_xform\n",
    "    #     wrist_pos = wrist_xform[:3, 3]\n",
    "    #     print(wrist_pos) \n",
    "    #     ax.scatter(*wrist_pos, color='blue', s=50, label=f'Wrist {hand_idx}')\n",
    "\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.set_title(f\"Camera Positions and Orientations in World Coordinate System (Frame {frame_idx})\")\n",
    "    ax.legend()\n",
    "\n",
    "    # Set equal aspect ratio for all axes\n",
    "    ax.set_box_aspect((np.ptp(ax.get_xlim()), np.ptp(ax.get_ylim()), np.ptp(ax.get_zlim())))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig_views, axs = plt.subplots(1, len(input_frame.views), figsize=(16, 4))\n",
    "    for vid, view in enumerate(input_frame.views):\n",
    "        axs[vid].imshow(view.image, cmap=\"gray\")\n",
    "        axs[vid].axis('off')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig, fig_views\n",
    "\n",
    "# Use observe instead of interactive\n",
    "def on_slider_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "            # matplotlib clear output\n",
    "            plt.close()\n",
    "            fig, fig_views = update_plot(change['new'])\n",
    "            plt.show()\n",
    "\n",
    "slider = IntSlider(\n",
    "    min=0,\n",
    "    max=len(image_pose_list) - 1,\n",
    "    step=1,\n",
    "    value=0,\n",
    "    description='Frame Index:'\n",
    ")\n",
    "\n",
    "# Observe slider changes\n",
    "slider.observe(on_slider_change)\n",
    "\n",
    "# Create an output widget to display the plot\n",
    "output = Output()\n",
    "\n",
    "# Display the slider and output\n",
    "display(slider, output)\n",
    "\n",
    "# Manually trigger the initial update\n",
    "with output:\n",
    "    #fig, fig_views = update_plot(0)\n",
    "    #plt.show()\n",
    "    pass\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize camera's position and orientation in world coordinate system using plotly\n",
    "import plotly.graph_objs as go\n",
    "import numpy as np\n",
    "from ipywidgets import interactive, IntSlider, Output\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def create_camera_visualization(camera_to_world_xf):\n",
    "    # Extract camera position (translation)\n",
    "    camera_pos = camera_to_world_xf[:3, 3]\n",
    "    \n",
    "    # Extract camera orientation (rotation)\n",
    "    camera_x = camera_to_world_xf[:3, 0]\n",
    "    camera_y = camera_to_world_xf[:3, 1]\n",
    "    camera_z = camera_to_world_xf[:3, 2]\n",
    "    \n",
    "    # Create camera position marker\n",
    "    camera_marker = go.Scatter3d(\n",
    "        x=[camera_pos[0]], y=[camera_pos[1]], z=[camera_pos[2]],\n",
    "        mode='markers',\n",
    "        marker=dict(size=8, color='red'),\n",
    "        name='Camera'\n",
    "    )\n",
    "    \n",
    "    # Create camera orientation arrows\n",
    "    scale = 10  # Scale factor for orientation arrows\n",
    "    arrows = []\n",
    "    for axis, color in zip([camera_x, camera_y, camera_z], ['red', 'green', 'blue']):\n",
    "        arrows.append(go.Scatter3d(\n",
    "            x=[camera_pos[0], camera_pos[0] + scale * axis[0]],\n",
    "            y=[camera_pos[1], camera_pos[1] + scale * axis[1]],\n",
    "            z=[camera_pos[2], camera_pos[2] + scale * axis[2]],\n",
    "            mode='lines',\n",
    "            line=dict(color=color, width=3),\n",
    "            name=f'{color.capitalize()} axis'\n",
    "        ))\n",
    "    \n",
    "    return [camera_marker] + arrows\n",
    "\n",
    "# Initialize an empty figure\n",
    "fig = go.Figure()\n",
    "\n",
    "def update_plot(frame_idx):\n",
    "    input_frame, gt_tracking = image_pose_list[frame_idx]\n",
    "    \n",
    "    # Clear previous traces\n",
    "    fig.data = []\n",
    "    \n",
    "    # Add camera visualizations for each view\n",
    "    for i, view in enumerate(input_frame.views):\n",
    "        camera_vis = create_camera_visualization(\n",
    "            view.camera.camera_to_world_xf\n",
    "        )\n",
    "        for trace in camera_vis:\n",
    "            fig.add_trace(trace)\n",
    "\n",
    "    # wrist_xform : root-to-world wrist transform\n",
    "    # assume that the unit is millimeter\n",
    "    for hand_idx, gt_hand_pose in gt_tracking.items():\n",
    "        wrist_xform = gt_hand_pose.wrist_xform\n",
    "        wrist_pos = wrist_xform[:3, 3]\n",
    "        \n",
    "        # Add wrist marker\n",
    "        wrist_marker = go.Scatter3d(\n",
    "            x=[wrist_pos[0]], y=[wrist_pos[1]], z=[wrist_pos[2]],\n",
    "            mode='markers',\n",
    "            marker=dict(size=8, color='blue'),\n",
    "            name=f'Wrist {hand_idx}'\n",
    "        )\n",
    "        fig.add_trace(wrist_marker)\n",
    "        \n",
    "        # Add 3D hand keypoints\n",
    "        \n",
    "        keypoints = landmarks_from_hand_pose(\n",
    "            image_pose_stream._hand_pose_labels.hand_model,\n",
    "            gt_hand_pose,\n",
    "            hand_idx\n",
    "        )\n",
    "        x_coords, y_coords, z_coords = [], [], []\n",
    "        for kp in keypoints:\n",
    "            x_coords.append(kp[0])\n",
    "            y_coords.append(kp[1])\n",
    "            z_coords.append(kp[2])\n",
    "        \n",
    "        keypoints_scatter = go.Scatter3d(\n",
    "            x=x_coords, y=y_coords, z=z_coords,\n",
    "            mode='markers',\n",
    "            marker=dict(size=5, color='green' if hand_idx == 0 else 'red'),\n",
    "            name=f'Hand {hand_idx} Keypoints'\n",
    "        )\n",
    "        fig.add_trace(keypoints_scatter)\n",
    "        \n",
    "        # Add lines connecting keypoints (you may need to adjust this based on your keypoint order)\n",
    "        for i in range(len(keypoints) - 1):\n",
    "            line = go.Scatter3d(\n",
    "                x=[keypoints[i][0], keypoints[i+1][0]],\n",
    "                y=[keypoints[i][1], keypoints[i+1][1]],\n",
    "                z=[keypoints[i][2], keypoints[i+1][2]],\n",
    "                mode='lines',\n",
    "                line=dict(color='cyan', width=2),\n",
    "                name=f'Hand {hand_idx} Bone'\n",
    "            )\n",
    "            fig.add_trace(line)\n",
    "\n",
    "    # Set layout\n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis_title='X',\n",
    "            yaxis_title='Y',\n",
    "            zaxis_title='Z',\n",
    "            aspectmode='data'\n",
    "        ),\n",
    "        title=f\"Camera Positions, Hand Keypoints, and Orientations in World Coordinate System (Frame {frame_idx})\",\n",
    "        legend_title=\"Legend\",\n",
    "        height=800,\n",
    "        width=1000\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "INIT_FRAME_IDX = 300\n",
    "\n",
    "# Create a slider widget\n",
    "slider = IntSlider(\n",
    "    min=0,\n",
    "    max=len(image_pose_list) - 1,\n",
    "    step=1,\n",
    "    value=INIT_FRAME_IDX,\n",
    "    description='Frame Index:'\n",
    ")\n",
    "\n",
    "# Create an output widget to display the plot\n",
    "output = Output()\n",
    "\n",
    "# Create an interactive widget\n",
    "def update_output(frame_idx):\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "        fig = update_plot(frame_idx)\n",
    "        fig.show()\n",
    "        \n",
    "        plt.figure(figsize=(30, 10))\n",
    "        for vid, view in enumerate(\n",
    "            image_pose_list[frame_idx][0].views\n",
    "        ):\n",
    "            plt.subplot(1, len(input_frame.views), vid+1)\n",
    "            plt.imshow(view.image, cmap=\"gray\")\n",
    "        plt.show()\n",
    "        \n",
    "interactive_plot = interactive(update_output, frame_idx=slider)\n",
    "\n",
    "# Display the interactive widget and output\n",
    "display(interactive_plot, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize camera's position and orientation in world coordinate system using plotly\n",
    "import plotly.graph_objs as go\n",
    "import numpy as np\n",
    "from ipywidgets import interactive, IntSlider, Output\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def create_camera_visualization(camera_to_world_xf):\n",
    "    # Extract camera position (translation)\n",
    "    camera_pos = camera_to_world_xf[:3, 3]\n",
    "    \n",
    "    # Extract camera orientation (rotation)\n",
    "    camera_x = camera_to_world_xf[:3, 0]\n",
    "    camera_y = camera_to_world_xf[:3, 1]\n",
    "    camera_z = camera_to_world_xf[:3, 2]\n",
    "    \n",
    "    # Create camera position marker\n",
    "    camera_marker = go.Scatter3d(\n",
    "        x=[camera_pos[0]], y=[camera_pos[1]], z=[camera_pos[2]],\n",
    "        mode='markers',\n",
    "        marker=dict(size=8, color='red'),\n",
    "        name='Camera'\n",
    "    )\n",
    "    \n",
    "    # Create camera orientation arrows\n",
    "    scale = 10  # Scale factor for orientation arrows\n",
    "    arrows = []\n",
    "    for axis, color in zip([camera_x, camera_y, camera_z], ['red', 'green', 'blue']):\n",
    "        arrows.append(go.Scatter3d(\n",
    "            x=[camera_pos[0], camera_pos[0] + scale * axis[0]],\n",
    "            y=[camera_pos[1], camera_pos[1] + scale * axis[1]],\n",
    "            z=[camera_pos[2], camera_pos[2] + scale * axis[2]],\n",
    "            mode='lines',\n",
    "            line=dict(color=color, width=3),\n",
    "            name=f'{color.capitalize()} axis'\n",
    "        ))\n",
    "    \n",
    "    return [camera_marker] + arrows\n",
    "\n",
    "# Initialize an empty figure\n",
    "fig = go.Figure()\n",
    "\n",
    "def update_plot(frame_idx):\n",
    "    input_frame, gt_tracking = image_pose_list[frame_idx]\n",
    "    \n",
    "    # Clear previous traces\n",
    "    fig.data = []\n",
    "    \n",
    "    # Add camera visualizations for each view\n",
    "    for i, view in enumerate(input_frame.views):\n",
    "        camera_vis = create_camera_visualization(\n",
    "            view.camera.camera_to_world_xf\n",
    "        )\n",
    "        for trace in camera_vis:\n",
    "            fig.add_trace(trace)\n",
    "\n",
    "    # wrist_xform : root-to-world wrist transform\n",
    "    # assume that the unnit is millimeter\n",
    "    for hand_idx, gt_hand_pose in gt_tracking.items() :\n",
    "        wrist_xform = gt_hand_pose.wrist_xform\n",
    "        wrist_pos = wrist_xform[:3, 3]\n",
    "        print(wrist_pos)\n",
    "        print()\n",
    "        wrist_x = wrist_xform[:3, 0]\n",
    "        wrist_y = wrist_xform[:3, 1]\n",
    "        wrist_z = wrist_xform[:3, 2]\n",
    "        \n",
    "        wrist_marker = go.Scatter3d(\n",
    "            x=[wrist_pos[0]], y=[wrist_pos[1]], z=[wrist_pos[2]],\n",
    "            mode='markers',\n",
    "            marker=dict(size=8, color='blue'),\n",
    "            name=f'Wrist {hand_idx}'\n",
    "        )\n",
    "        fig.add_trace(wrist_marker)\n",
    "        \n",
    "\n",
    "    # Set layout\n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis_title='X',\n",
    "            yaxis_title='Y',\n",
    "            zaxis_title='Z',\n",
    "            aspectmode='data'\n",
    "        ),\n",
    "        title=f\"Camera Positions and Orientations in World Coordinate System (Frame {frame_idx})\",\n",
    "        legend_title=\"Legend\",\n",
    "        height=800,\n",
    "        width=1000\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "INIT_FRAME_IDX = 100\n",
    "\n",
    "# Create a slider widget\n",
    "slider = IntSlider(\n",
    "    min=0,\n",
    "    max=len(image_pose_list) - 1,\n",
    "    step=1,\n",
    "    value=INIT_FRAME_IDX,\n",
    "    description='Frame Index:'\n",
    ")\n",
    "\n",
    "# Create an output widget to display the plot\n",
    "output = Output()\n",
    "\n",
    "# Create an interactive widget\n",
    "def update_output(frame_idx):\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "        fig = update_plot(frame_idx)\n",
    "        fig.show()\n",
    "        \n",
    "        plt.figure(figsize=(30, 10))\n",
    "        for vid, view in enumerate(\n",
    "            image_pose_list[frame_idx][0].views\n",
    "        ):\n",
    "            plt.subplot(1, len(input_frame.views), vid+1)\n",
    "            plt.imshow(view.image, cmap=\"gray\")\n",
    "        plt.show()\n",
    "        \n",
    "interactive_plot = interactive(update_output, frame_idx=slider)\n",
    "\n",
    "# Display the interactive widget and output\n",
    "display(interactive_plot, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for (input_frame, gt_tracking) in image_pose_list :\n",
    "    # check if Z axis of second and third camera are parallel\n",
    "    \n",
    "    print(\n",
    "        \"cosine between Z axis of second and third camera : \",\n",
    "        #np.sqrt(np.sum(np.square(input_frame.views[1].camera.camera_to_world_xf[:3, 2]))),\n",
    "        #np.sqrt(np.sum(np.square(input_frame.views[2].camera.camera_to_world_xf[:3, 2]))),\n",
    "        np.dot(\n",
    "            input_frame.views[1].camera.camera_to_world_xf[:3, 2],\n",
    "            input_frame.views[2].camera.camera_to_world_xf[:3, 2]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    \n",
    "    print(\n",
    "        \"cosine between Z axis of first camera and second camera : \",\n",
    "        np.dot(\n",
    "            input_frame.views[0].camera.camera_to_world_xf[:3, 2],\n",
    "            input_frame.views[1].camera.camera_to_world_xf[:3, 2]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(np.dot(\n",
    "        np.cross(\n",
    "            input_frame.views[0].camera.camera_to_world_xf[:3, 2],\n",
    "            input_frame.views[1].camera.camera_to_world_xf[:3, 2],\n",
    "        ),\n",
    "        np.cross(\n",
    "            input_frame.views[2].camera.camera_to_world_xf[:3, 2],\n",
    "            input_frame.views[3].camera.camera_to_world_xf[:3, 2],\n",
    "        )\n",
    "    ))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model in dataset\n",
    "- Visualize warping and hand flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"pretrained_weights.torch\"\n",
    "model_path = os.path.join(\".\", \"pretrained_models\", model_name)\n",
    "\n",
    "model = load_pretrained_model(model_path)\n",
    "model.eval()\n",
    "tracker = HandTracker(model, HandTrackerOpts())\n",
    "\n",
    "model_orig = load_pretrained_model(model_path)\n",
    "model_orig.eval()\n",
    "tracker_orig = HandTracker(model, HandTrackerOpts())\n",
    "\n",
    "\n",
    "FRAME_IDX = 29\n",
    "\n",
    "input_frame, gt_tracking = image_pose_list[FRAME_IDX]\n",
    "\n",
    "\n",
    "crop_cameras = tracker.gen_crop_cameras(\n",
    "    [view.camera for view in input_frame.views],\n",
    "    image_pose_stream._hand_pose_labels.camera_angles,\n",
    "    hand_model,\n",
    "    gt_tracking,\n",
    "    min_num_crops=1,\n",
    ")\n",
    "\n",
    "# ==============================================\n",
    "# visualizes gt_hand_pose and whole image\n",
    "# for each camera, find if hand is projected to this camera\n",
    "# if so, visualize the keypoints on the image\n",
    "for camera_idx in range(len(input_frame.views)) :\n",
    "    orig_camera = input_frame.views[camera_idx].camera\n",
    "    keypoints_window_list = []\n",
    "    # find if crop_camera is generated to this camera\n",
    "    for hand_idx, crop_camera_dict in crop_cameras.items() :\n",
    "        if camera_idx in crop_camera_dict.keys() :\n",
    "            hand_pose = gt_tracking[hand_idx]\n",
    "            keypoints_world = landmarks_from_hand_pose(\n",
    "                hand_model,\n",
    "                hand_pose,\n",
    "                hand_idx\n",
    "            )\n",
    "            keypoints_eye = orig_camera.world_to_eye(keypoints_world)\n",
    "            keypoints_window = orig_camera.eye_to_window(keypoints_eye)\n",
    "            keypoints_window_list.append(keypoints_window)\n",
    "    \n",
    "    color_list = ['r', 'g', 'b', 'y', 'm', 'c'] \n",
    "    if len(keypoints_window_list) > 0 :\n",
    "        plt.imshow(input_frame.views[camera_idx].image, cmap=\"gray\")\n",
    "        for i in range(len(keypoints_window_list)) :\n",
    "            plt.scatter(\n",
    "                keypoints_window_list[i][:, 0],\n",
    "                keypoints_window_list[i][:, 1],\n",
    "                color=color_list[i],\n",
    "                label='Keypoint'\n",
    "            )\n",
    "        plt.show()\n",
    "\n",
    "res = tracker.track_frame_analysis(\n",
    "    input_frame, hand_model, crop_cameras, gt_tracking\n",
    ")\n",
    "res_orig = tracker_orig.track_frame(\n",
    "    input_frame, hand_model, crop_cameras\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)\n",
    "print()\n",
    "print(res_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to run mediapipe hand model on datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mediapie does not work well on monochrome image\n",
    "_ = \"\"\"\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands = 2,\n",
    "    \n",
    "    model_complexity = 1,\n",
    "    min_detection_confidence = 0.3, \n",
    "    min_tracking_confidence = 0.3\n",
    ")\n",
    "\n",
    "def visualize_frame(frame_idx):\n",
    "    input_frame, gt_tracking = image_pose_list[frame_idx]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, frame in enumerate(input_frame.views):\n",
    "        frame_rgb = cv2.cvtColor(frame.image, cv2.COLOR_GRAY2RGB)\n",
    "        axes[i].imshow(frame_rgb)\n",
    "        axes[i].set_title(f\"View {i+1}\")\n",
    "        \n",
    "        mp_result = hands.process(frame_rgb)\n",
    "        if mp_result.multi_handedness:\n",
    "            axes[i].set_xlabel(f\"{len(mp_result.multi_handedness)} Hand detected\")\n",
    "            for hand_landmarks in mp_result.multi_hand_landmarks:\n",
    "                hand_landmarks_norm_np = np.array(list(map(\n",
    "                    lambda l : [l.x, l.y, l.z],\n",
    "                    hand_landmarks.landmark\n",
    "                )))\n",
    "                hand_landmarks_np = hand_landmarks_norm_np * np.array([frame.camera.width, frame.camera.height, 1])\n",
    "                \n",
    "                axes[i].scatter(\n",
    "                    hand_landmarks_np[:, 0],\n",
    "                    hand_landmarks_np[:, 1],\n",
    "                    color='blue',\n",
    "                    label='Keypoint'\n",
    "                )\n",
    "        else:\n",
    "            axes[i].set_xlabel(\"No hand detected\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "slider = IntSlider(\n",
    "    min=0,\n",
    "    max=len(image_pose_list) - 1,\n",
    "    step=1,\n",
    "    value=0,\n",
    "    description='Frame Index:'\n",
    ")\n",
    "\n",
    "output = Output()\n",
    "\n",
    "interactive_plot = interactive(visualize_frame, frame_idx=slider)\n",
    "display(interactive_plot, output)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play with Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"pretrained_weights.torch\"\n",
    "model_path = os.path.join(\".\", \"pretrained_models\", model_name)\n",
    "model = load_pretrained_model(model_path)\n",
    "model.eval()\n",
    "tracker = HandTracker(model, HandTrackerOpts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = HandTracker(model, HandTrackerOpts())\n",
    "crop_cameras = tracker.gen_crop_cameras(\n",
    "    [view.camera for view in input_frame.views],\n",
    "    image_pose_stream._hand_pose_labels.camera_angles,\n",
    "    hand_model,\n",
    "    gt_tracking,\n",
    "    min_num_crops=1,\n",
    ")\n",
    "crop_cameras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = HandTracker(model, HandTrackerOpts())\n",
    "\n",
    "crop_cameras = tracker.gen_crop_cameras_analysis(\n",
    "    #[view.camera for view in input_frame.views],\n",
    "    input_frame     = input_frame,\n",
    "    camera_angles   = image_pose_stream._hand_pose_labels.camera_angles,\n",
    "    hand_model      = hand_model,\n",
    "    gt_tracking     = gt_tracking,\n",
    "    min_num_crops   = 1,\n",
    ")\n",
    "\n",
    "print(crop_cameras)\n",
    "print()\n",
    "\n",
    "crop_cameras = tracker.gen_crop_cameras(\n",
    "    [view.camera for view in input_frame.views],\n",
    "    image_pose_stream._hand_pose_labels.camera_angles,\n",
    "    hand_model,\n",
    "    gt_tracking,\n",
    "    min_num_crops=1,\n",
    ")\n",
    "print(crop_cameras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = tracker.track_frame(input_frame, hand_model, crop_cameras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put ELP Stereo Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMG_WIDTH = 640\n",
    "IMG_HEIGHT = 480\n",
    "\n",
    "left_to_right_r = np.array([\n",
    "    9.9997658245714527e-01, 5.5910744958795095e-04, 6.8206990981942916e-03,\n",
    "    -5.4903304536865717e-04, 9.9999875583076248e-01, -1.4788169738349651e-03,\n",
    "    -6.8215174296769373e-03, 1.4750375543776898e-03, 9.9997564528550886e-01\n",
    "]).reshape(3, 3)\n",
    "\n",
    "left_to_right_t = np.array([\n",
    "    -5.9457914254177978e-02, -6.8318101539255457e-05, -1.8101725187729225e-04\n",
    "])\n",
    "\n",
    "# k1, k2, k3, k4, p1, p2, k5, k6\n",
    "distortion_coeffs_left = (\n",
    "    -3.7539305827469560e-02, \n",
    "    -8.7553205432575471e-03,\n",
    "    2.2015408171895236e-03, \n",
    "    -6.6218076061138698e-04,\n",
    "    0, 0, 0, 0\n",
    ")\n",
    "camera_to_world_xf_left = np.eye(4)\n",
    "rotation_left = np.array([\n",
    "    [9.9997658245714527e-01,  5.5910744958795095e-04,  6.8206990981942916e-03,],\n",
    "    [-5.4903304536865717e-04, 9.9999875583076248e-01, -1.4788169738349651e-03,],\n",
    "    [-6.8215174296769373e-03, 1.4750375543776898e-03,  9.9997564528550886e-01 ],\n",
    "]).reshape(3, 3)\n",
    "camera_to_world_xf_left[:3, :3] = rotation_left\n",
    "#camera_to_world_xf_left[:3, 3] = [\n",
    "cam_left = Fisheye62CameraModel(\n",
    "    width   = IMG_WIDTH,\n",
    "    height  = IMG_HEIGHT,\n",
    "    f       = (2.3877057700850656e+02, 2.3903223316525276e+02),\n",
    "    c       = (3.1846939219741773e+02, 2.4685137381795201e+02),\n",
    "    distort_coeffs = distortion_coeffs_left,\n",
    "    camera_to_world_xf = np.eye(4)\n",
    ")\n",
    "\n",
    "\n",
    "distortion_coeffs_right = (\n",
    "    -3.6790400486095221e-02, \n",
    "    -8.2041573433038941e-03,\n",
    "    1.0552974220937024e-03, \n",
    "    -2.5841665172692902e-04,\n",
    "    0, 0, 0, 0\n",
    ")\n",
    "camera_to_world_xf_right = np.eye(4)\n",
    "rotation_right = np.array([\n",
    "    [9.9999470555416226e-01, 1.1490100298631428e-03, 3.0444440536135159e-03,],\n",
    "    [-1.1535052313709361e-03, 9.9999824663038117e-01, 1.4751819698614872e-03,],\n",
    "    [-3.0427437166985561e-03, -1.4786859417328980e-03, 9.9999427758290704e-01 ],\n",
    "]).reshape(3, 3)\n",
    "camera_to_world_xf_right[:3, :3] = rotation_right\n",
    "camera_to_world_xf_right[:3, 3] = left_to_right_t\n",
    "#camera_to_world_xf_right[:3, 3] = [\n",
    "cam_right = Fisheye62CameraModel(\n",
    "    width   = IMG_WIDTH,\n",
    "    height  = IMG_HEIGHT,\n",
    "    f       = (2.3952183485043457e+02, 2.3981379751051574e+02),\n",
    "    c       = (3.1286224145189811e+02, 2.5158397962108106e+02),\n",
    "    distort_coeffs = distortion_coeffs_right,\n",
    "    camera_to_world_xf = camera_to_world_xf_right\n",
    ")\n",
    "\n",
    "def open_stereo_camera(IMAGE_WIDTH, IMAGE_HEIGHT, CAM_ID_MAX = 10) :\n",
    "    for CAM_ID in range(-1, CAM_ID_MAX) :\n",
    "        cap = cv2.VideoCapture(CAM_ID)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, IMAGE_WIDTH * 2)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, IMAGE_HEIGHT)\n",
    "        if cap.isOpened() :\n",
    "            print(f\"Camera ID {CAM_ID} Frame Width {cap.get(cv2.CAP_PROP_FRAME_WIDTH)}\")\n",
    "            return cap\n",
    "\n",
    "CAM_ID_MAX = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands_left_cam = mp_hands.Hands(\n",
    "    max_num_hands = 2,\n",
    "    model_complexity = 1,\n",
    "    min_detection_confidence = 0.3, \n",
    "    min_tracking_confidence = 0.3\n",
    ")\n",
    "\n",
    "hands_right_cam = mp_hands.Hands(\n",
    "    max_num_hands = 2,   \n",
    "    model_complexity = 1,\n",
    "    min_detection_confidence = 0.3, \n",
    "    min_tracking_confidence = 0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = open_stereo_camera(IMG_WIDTH, IMG_HEIGHT, CAM_ID_MAX)\n",
    "ret, frame_stereo = cap.read()\n",
    "frame_stereo = cv2.cvtColor(frame_stereo, cv2.COLOR_BGR2RGB)\n",
    "frame_left = frame_stereo[:, :IMG_WIDTH]\n",
    "frame_right = frame_stereo[:, IMG_WIDTH:]\n",
    "frame_right_gray = cv2.cvtColor(frame_right, cv2.COLOR_RGB2GRAY)\n",
    "frame_left_gray = cv2.cvtColor(frame_left, cv2.COLOR_RGB2GRAY)\n",
    "cap.release()\n",
    "\n",
    "hand_pose_result_left_cam = hands_left_cam.process(frame_left)\n",
    "# key : handedness index\n",
    "# value : window coordinate hand pose\n",
    "if hand_pose_result_left_cam.multi_handedness :\n",
    "    hand_pose_window_left_cam : Dict[int, np.ndarray] = dict(zip(\n",
    "        list(map(\n",
    "            lambda x : x.classification[0].index,\n",
    "            hand_pose_result_left_cam.multi_handedness\n",
    "        )),\n",
    "        list(map(\n",
    "            lambda landamrk_per_hand : np.array(list(map(\n",
    "                lambda l : [l.x, l.y, l.z],\n",
    "                landamrk_per_hand.landmark\n",
    "            ))) * np.array([frame_left.shape[1], frame_left.shape[0], 1]),\n",
    "            hand_pose_result_left_cam.multi_hand_landmarks\n",
    "        ))\n",
    "    ))\n",
    "print(\"left cam detected hand index : \", *hand_pose_window_left_cam.keys())\n",
    "\n",
    "hand_pose_result_right_cam = hands_right_cam.process(frame_right)\n",
    "if hand_pose_result_right_cam.multi_handedness :\n",
    "    hand_pose_window_right_cam : Dict[int, np.ndarray] = dict(zip(\n",
    "        list(map(\n",
    "            lambda x : x.classification[0].index,\n",
    "            hand_pose_result_right_cam.multi_handedness\n",
    "        )),\n",
    "        list(map(\n",
    "            lambda landamrk_per_hand : np.array(list(map(\n",
    "                lambda l : [l.x, l.y, l.z],\n",
    "                landamrk_per_hand.landmark\n",
    "            ))) * np.array([frame_right.shape[1], frame_right.shape[0], 1]),\n",
    "            hand_pose_result_right_cam.multi_hand_landmarks\n",
    "        ))\n",
    "    ))\n",
    "    hand_pose_window_right_cam\n",
    "print(\"right cam detected hand index : \", *hand_pose_window_right_cam.keys())\n",
    "\n",
    "VIS_HAND_IDX_LIST = [0, 1]\\\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(frame_left)\n",
    "for hand_idx in VIS_HAND_IDX_LIST :\n",
    "    if hand_idx in hand_pose_window_left_cam :\n",
    "        print(hand_idx)\n",
    "        plt.scatter(\n",
    "            hand_pose_window_left_cam[hand_idx][:, 0],\n",
    "            hand_pose_window_left_cam[hand_idx][:, 1],\n",
    "            color='blue',\n",
    "            label='Keypoint'\n",
    "        )\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(frame_right)\n",
    "for hand_idx in VIS_HAND_IDX_LIST :\n",
    "    if hand_idx in hand_pose_window_right_cam :\n",
    "        plt.scatter(\n",
    "            hand_pose_window_right_cam[hand_idx][:, 0],\n",
    "            hand_pose_window_right_cam[hand_idx][:, 1],\n",
    "            color='blue',\n",
    "            label='Keypoint'\n",
    "    )   \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"pretrained_weights.torch\"\n",
    "model_path = os.path.join(\".\", \"pretrained_models\", model_name)\n",
    "model = load_pretrained_model(model_path)\n",
    "model.eval()\n",
    "tracker = HandTracker(model, HandTrackerOpts())\n",
    "\n",
    "crop_camera_dict = tracker.gen_crop_cameras_from_stereo_camera_with_window_hand_pose(\n",
    "    camera_left = cam_left,\n",
    "    camera_right = cam_right,\n",
    "    window_hand_pose_left = hand_pose_window_left_cam,\n",
    "    window_hand_pose_right = hand_pose_window_right_cam\n",
    ")\n",
    "crop_camera_dict\n",
    "\n",
    "fisheye_stereo_input_frame = InputFrame(\n",
    "    views = [\n",
    "        ViewData(\n",
    "            image = frame_left_gray,\n",
    "            camera = cam_left,\n",
    "            camera_angle = 0,\n",
    "        ),\n",
    "        ViewData(\n",
    "            image = frame_right_gray,\n",
    "            camera = cam_right,\n",
    "            camera_angle = 0,\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "tracker.visualize_crop_cameras(\n",
    "    input_frame = fisheye_stereo_input_frame,\n",
    "    window_hand_pose_left = hand_pose_window_left_cam,\n",
    "    window_hand_pose_right = hand_pose_window_right_cam,\n",
    "    crop_camera_dict = crop_camera_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fisheye_stereo_input_frame.views[0].camera)\n",
    "print(fisheye_stereo_input_frame.views[0].camera.camera_to_world_xf)\n",
    "\n",
    "print()\n",
    "\n",
    "print(fisheye_stereo_input_frame.views[1].camera)\n",
    "print(fisheye_stereo_input_frame.views[1].camera.camera_to_world_xf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = tracker.track_frame_analysis(fisheye_stereo_input_frame, hand_model, crop_camera_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracked_keypoints_dict = {}\n",
    "for hand_idx in res.hand_poses.keys() :\n",
    "    tracked_keypoints = landmarks_from_hand_pose(\n",
    "        hand_model, res.hand_poses[hand_idx], hand_idx\n",
    "    )\n",
    "    tracked_keypoints_dict[hand_idx] = tracked_keypoints\n",
    "\n",
    "camera_left_pose = cam_left.camera_to_world_xf[:3, 3] * M_TO_MM\n",
    "camera_left_x_axis = cam_left.camera_to_world_xf[:3, 0] * M_TO_MM\n",
    "camera_left_y_axis = cam_left.camera_to_world_xf[:3, 1] * M_TO_MM \n",
    "camera_left_z_axis = cam_left.camera_to_world_xf[:3, 2] * M_TO_MM\n",
    "\n",
    "camera_right_pose = cam_right.camera_to_world_xf[:3, 3] * M_TO_MM\n",
    "camera_right_x_axis = cam_right.camera_to_world_xf[:3, 0] * M_TO_MM\n",
    "camera_right_y_axis = cam_right.camera_to_world_xf[:3, 1] * M_TO_MM\n",
    "camera_right_z_axis = cam_right.camera_to_world_xf[:3, 2] * M_TO_MM\n",
    "\n",
    "print(camera_left_pose)\n",
    "print(camera_right_pose)\n",
    "# visualize tracked_keypoints_dict, camera_left, camera_right using plotly\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x = tracked_keypoints_dict[0][:, 0],\n",
    "    y = tracked_keypoints_dict[0][:, 1],\n",
    "    z = tracked_keypoints_dict[0][:, 2],\n",
    "    mode = 'markers',\n",
    "    marker = dict(\n",
    "        size = 5,\n",
    "        color = 'blue',\n",
    "        opacity = 0.5\n",
    "    )\n",
    "))\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x = tracked_keypoints_dict[1][:, 0],\n",
    "    y = tracked_keypoints_dict[1][:, 1],\n",
    "    z = tracked_keypoints_dict[1][:, 2],\n",
    "    mode = 'markers',\n",
    "    marker = dict(\n",
    "        size = 5,\n",
    "        color = 'red',\n",
    "        opacity = 0.5\n",
    "    )\n",
    "))\n",
    "\n",
    "# add camera axis \n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x = [0, camera_left_x_axis[0]],\n",
    "    y = [0, camera_left_x_axis[1]],\n",
    "    z = [0, camera_left_x_axis[2]],\n",
    "    mode = 'lines',\n",
    "    line = dict(\n",
    "        color = 'red',\n",
    "        width = 5\n",
    "    )\n",
    "))\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x = [0, camera_left_y_axis[0]],\n",
    "    y = [0, camera_left_y_axis[1]],\n",
    "    z = [0, camera_left_y_axis[2]],\n",
    "    mode = 'lines',\n",
    "    line = dict(\n",
    "        color = 'green',\n",
    "        width = 5\n",
    "    )\n",
    "))\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x = [0, camera_left_z_axis[0]],\n",
    "    y = [0, camera_left_z_axis[1]],\n",
    "    z = [0, camera_left_z_axis[2]],\n",
    "    mode = 'lines',\n",
    "    line = dict(\n",
    "        color = 'blue',\n",
    "        width = 5\n",
    "    )\n",
    "))\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x = [0, camera_right_x_axis[0]],\n",
    "    y = [0, camera_right_x_axis[1]],\n",
    "    z = [0, camera_right_x_axis[2]],\n",
    "    mode = 'lines',\n",
    "    line = dict(\n",
    "        color = 'red',\n",
    "        width = 5\n",
    "    )\n",
    "))\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x = [0, camera_right_y_axis[0]],\n",
    "    y = [0, camera_right_y_axis[1]],\n",
    "    z = [0, camera_right_y_axis[2]],\n",
    "    mode = 'lines',\n",
    "    line = dict(\n",
    "        color = 'green',\n",
    "        width = 5\n",
    "    )\n",
    "))\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x = [0, camera_right_z_axis[0]],\n",
    "    y = [0, camera_right_z_axis[1]],\n",
    "    z = [0, camera_right_z_axis[2]],\n",
    "    mode = 'lines',\n",
    "    line = dict(\n",
    "        color = 'blue',\n",
    "        width = 5\n",
    "    )\n",
    "))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joint Altogether\n",
    "\n",
    "import socket\n",
    "\n",
    "cap = open_stereo_camera(IMG_WIDTH, IMG_HEIGHT, CAM_ID_MAX)\n",
    "\n",
    "model_name = \"pretrained_weights.torch\"\n",
    "model_path = os.path.join(\".\", \"pretrained_models\", model_name)\n",
    "model = load_pretrained_model(model_path)\n",
    "model.eval()\n",
    "tracker = HandTracker(model, HandTrackerOpts())\n",
    "\n",
    "sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "serverAddressPort = (\"127.0.0.1\", 5052)\n",
    "\n",
    "idx = 0\n",
    "\n",
    "while True :\n",
    "    idx += 1\n",
    "    \n",
    "    ret, frame_stereo = cap.read()\n",
    "    frame_stereo = cv2.cvtColor(frame_stereo, cv2.COLOR_BGR2RGB)\n",
    "    frame_left = frame_stereo[:, :IMG_WIDTH]\n",
    "    frame_right = frame_stereo[:, IMG_WIDTH:]\n",
    "    frame_right_gray = cv2.cvtColor(frame_right, cv2.COLOR_RGB2GRAY)\n",
    "    frame_left_gray = cv2.cvtColor(frame_left, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    hand_pose_result_left_cam = hands_left_cam.process(frame_left)\n",
    "    if hand_pose_result_left_cam.multi_handedness :\n",
    "        hand_pose_window_left_cam : Dict[int, np.ndarray] = dict(zip(\n",
    "            list(map(\n",
    "                lambda x : x.classification[0].index,\n",
    "                hand_pose_result_left_cam.multi_handedness\n",
    "            )),\n",
    "            list(map(\n",
    "                lambda landamrk_per_hand : np.array(list(map(\n",
    "                    lambda l : [l.x, l.y, l.z],\n",
    "                    landamrk_per_hand.landmark\n",
    "                ))) * np.array([frame_left.shape[1], frame_left.shape[0], 1]),\n",
    "                hand_pose_result_left_cam.multi_hand_landmarks\n",
    "            ))\n",
    "        ))\n",
    "\n",
    "    hand_pose_result_right_cam = hands_right_cam.process(frame_right)\n",
    "    if hand_pose_result_right_cam.multi_handedness :\n",
    "        hand_pose_window_right_cam : Dict[int, np.ndarray] = dict(zip(\n",
    "            list(map(\n",
    "                lambda x : x.classification[0].index,\n",
    "                hand_pose_result_right_cam.multi_handedness\n",
    "            )),\n",
    "            list(map(\n",
    "                lambda landamrk_per_hand : np.array(list(map(\n",
    "                    lambda l : [l.x, l.y, l.z],\n",
    "                    landamrk_per_hand.landmark\n",
    "                ))) * np.array([frame_right.shape[1], frame_right.shape[0], 1]),\n",
    "                hand_pose_result_right_cam.multi_hand_landmarks\n",
    "            ))\n",
    "        ))\n",
    "        hand_pose_window_right_cam\n",
    "\n",
    "    fisheye_stereo_input_frame = InputFrame(\n",
    "        views = [\n",
    "            ViewData(\n",
    "                image = frame_left_gray,\n",
    "                camera = cam_left,\n",
    "                camera_angle = 0,\n",
    "            ),\n",
    "            ViewData(\n",
    "                image = frame_right_gray,\n",
    "                camera = cam_right,\n",
    "                camera_angle = 0,\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    crop_camera_dict = tracker.gen_crop_cameras_from_stereo_camera_with_window_hand_pose(\n",
    "        camera_left = cam_left,\n",
    "        camera_right = cam_right,\n",
    "        window_hand_pose_left = hand_pose_window_left_cam,\n",
    "        window_hand_pose_right = hand_pose_window_right_cam\n",
    "    )\n",
    "\n",
    "    res = tracker.track_frame(fisheye_stereo_input_frame, hand_model, crop_camera_dict)\n",
    "\n",
    "    tracked_keypoints_dict = {}\n",
    "    for hand_idx in res.hand_poses.keys() :\n",
    "        tracked_keypoints = landmarks_from_hand_pose(\n",
    "            hand_model, res.hand_poses[hand_idx], hand_idx\n",
    "        )\n",
    "        tracked_keypoints_dict[hand_idx] = tracked_keypoints\n",
    "\n",
    "    # sock.sendto(str.encode(str(data)), serverAddressPort)\n",
    "    \n",
    "    # for hand_idx, keypoints in tracked_keypoints_dict.items() :\n",
    "    #     print(hand_idx, keypoints.shape)\n",
    "    #     print(keypoints.tolist())\n",
    "    if 0 in tracked_keypoints_dict :\n",
    "        refined_keypoints = tracked_keypoints_dict[0] / 10 + 100\n",
    "        refined_keypoints[:, 1] = 200 - refined_keypoints[:, 1]\n",
    "        \n",
    "        content = str(refined_keypoints.reshape(-1).tolist())\n",
    "        print(content)\n",
    "        sock.sendto(str.encode(content), serverAddressPort)\n",
    "    \n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.arange(12).reshape(3, 4)\n",
    "print(arr)\n",
    "print(arr.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = list(range(10))\n",
    "ll.extend([11, 12, 13])\n",
    "print(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_keypoints.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umetrack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
